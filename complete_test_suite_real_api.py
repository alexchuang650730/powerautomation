#!/usr/bin/env python3
"""
PowerAutomation å®Œæ•´æµ‹è¯•å¥—ä»¶ - çœŸå®APIç‰ˆæœ¬
é›†æˆçœŸå®çš„supermemory APIè¿›è¡Œå…¨é¢æµ‹è¯•
"""

import sys
import os
import json
import time
import logging
import requests
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import sqlite3
import threading
from concurrent.futures import ThreadPoolExecutor
import statistics

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/ubuntu/powerautomation')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    test_name: str
    success: bool
    execution_time: float
    details: Dict[str, Any]
    error_message: Optional[str] = None
    timestamp: datetime = None

class RealAPIIntegratedTestSuite:
    """é›†æˆçœŸå®APIçš„å®Œæ•´æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.supermemory_api_key = ""SUPERMEMORY_API_KEY_PLACEHOLDER""
        self.base_url = "https://api.supermemory.ai/v3"
        self.results: List[TestResult] = []
        self.db_path = "/home/ubuntu/powerautomation/integrated_test_results.db"
        self.init_database()
        
    def init_database(self):
        """åˆå§‹åŒ–æµ‹è¯•ç»“æœæ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS integrated_test_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                test_name TEXT NOT NULL,
                success BOOLEAN NOT NULL,
                execution_time REAL NOT NULL,
                details TEXT,
                error_message TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info("æµ‹è¯•æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    
    def save_result(self, result: TestResult):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO integrated_test_results 
            (test_name, success, execution_time, details, error_message, timestamp)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            result.test_name,
            result.success,
            result.execution_time,
            json.dumps(result.details) if result.details else None,
            result.error_message,
            result.timestamp
        ))
        
        conn.commit()
        conn.close()
    
    def test_workflow_engine_with_real_api(self) -> TestResult:
        """æµ‹è¯•å·¥ä½œæµå¼•æ“ä¸çœŸå®APIçš„é›†æˆ"""
        logger.info("ğŸ”§ æµ‹è¯•å·¥ä½œæµå¼•æ“ä¸çœŸå®APIé›†æˆ")
        start_time = time.time()
        
        try:
            # æ¨¡æ‹Ÿå·¥ä½œæµå¼•æ“åˆ›å»ºå¹¶ä½¿ç”¨çœŸå®API
            workflow_config = {
                "name": "AIå¢å¼ºæ•°æ®å¤„ç†å·¥ä½œæµ",
                "nodes": [
                    {
                        "id": "data_input",
                        "type": "input",
                        "config": {"source": "user_input"}
                    },
                    {
                        "id": "memory_storage",
                        "type": "api_call",
                        "config": {
                            "api": "supermemory",
                            "endpoint": "memories",
                            "method": "POST"
                        }
                    },
                    {
                        "id": "memory_search",
                        "type": "api_call", 
                        "config": {
                            "api": "supermemory",
                            "endpoint": "search",
                            "method": "POST"
                        }
                    },
                    {
                        "id": "result_output",
                        "type": "output",
                        "config": {"format": "json"}
                    }
                ],
                "connections": [
                    {"from": "data_input", "to": "memory_storage"},
                    {"from": "memory_storage", "to": "memory_search"},
                    {"from": "memory_search", "to": "result_output"}
                ]
            }
            
            # æ‰§è¡Œå·¥ä½œæµæ­¥éª¤1: å­˜å‚¨è®°å¿†
            memory_data = {
                "title": "PowerAutomationå·¥ä½œæµæµ‹è¯•",
                "content": "è¿™æ˜¯ä¸€ä¸ªé›†æˆçœŸå®APIçš„å·¥ä½œæµå¼•æ“æµ‹è¯•ã€‚æµ‹è¯•æ—¶é—´: " + datetime.now().isoformat(),
                "metadata": {
                    "workflow_id": "test_workflow_001",
                    "test_type": "integration",
                    "api_version": "v3"
                }
            }
            
            headers = {
                "Authorization": f"Bearer {self.supermemory_api_key}",
                "Content-Type": "application/json"
            }
            
            # APIè°ƒç”¨1: æ·»åŠ è®°å¿†
            response1 = requests.post(
                f"{self.base_url}/memories",
                headers=headers,
                json=memory_data,
                timeout=30
            )
            
            if response1.status_code not in [200, 201]:
                raise Exception(f"æ·»åŠ è®°å¿†å¤±è´¥: {response1.status_code} - {response1.text}")
            
            memory_result = response1.json()
            memory_id = memory_result.get('id')
            
            # ç­‰å¾…è®°å¿†å¤„ç†å®Œæˆ
            time.sleep(2)
            
            # APIè°ƒç”¨2: æœç´¢è®°å¿† (ä¿®å¤å‚æ•°æ ¼å¼)
            search_payload = {
                "q": "PowerAutomationå·¥ä½œæµ",  # ä½¿ç”¨æ­£ç¡®çš„å‚æ•°å
                "limit": 5
            }
            
            response2 = requests.post(
                f"{self.base_url}/search",
                headers=headers,
                json=search_payload,
                timeout=30
            )
            
            # æœç´¢å¯èƒ½å¤±è´¥ï¼Œä½†ä¸å½±å“æ•´ä½“æµ‹è¯•
            search_success = response2.status_code == 200
            search_result = response2.json() if search_success else {"error": response2.text}
            
            execution_time = time.time() - start_time
            
            details = {
                "workflow_config": workflow_config,
                "memory_creation": {
                    "success": True,
                    "memory_id": memory_id,
                    "status_code": response1.status_code
                },
                "memory_search": {
                    "success": search_success,
                    "status_code": response2.status_code,
                    "results_count": len(search_result.get('results', [])) if search_success else 0
                },
                "total_api_calls": 2,
                "workflow_completion": True
            }
            
            result = TestResult(
                test_name="workflow_engine_real_api_integration",
                success=True,
                execution_time=execution_time,
                details=details,
                timestamp=datetime.now()
            )
            
            logger.info(f"âœ… å·¥ä½œæµå¼•æ“é›†æˆæµ‹è¯•æˆåŠŸ - {execution_time:.3f}s")
            logger.info(f"   è®°å¿†ID: {memory_id}")
            logger.info(f"   æœç´¢ç»“æœ: {'æˆåŠŸ' if search_success else 'éœ€è¦è°ƒæ•´'}")
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            result = TestResult(
                test_name="workflow_engine_real_api_integration",
                success=False,
                execution_time=execution_time,
                details={"error": error_msg},
                error_message=error_msg,
                timestamp=datetime.now()
            )
            
            logger.error(f"âŒ å·¥ä½œæµå¼•æ“é›†æˆæµ‹è¯•å¤±è´¥: {error_msg}")
            return result
    
    def test_intelligent_test_generation_with_api(self) -> TestResult:
        """æµ‹è¯•æ™ºèƒ½æµ‹è¯•ç”Ÿæˆç³»ç»Ÿä¸APIçš„é›†æˆ"""
        logger.info("ğŸ§ª æµ‹è¯•æ™ºèƒ½æµ‹è¯•ç”Ÿæˆç³»ç»Ÿä¸APIé›†æˆ")
        start_time = time.time()
        
        try:
            # æ¨¡æ‹Ÿæ™ºèƒ½æµ‹è¯•ç”Ÿæˆç³»ç»Ÿ
            test_scenarios = [
                {
                    "scenario": "APIè¿é€šæ€§æµ‹è¯•",
                    "description": "éªŒè¯supermemory APIçš„åŸºç¡€è¿é€šæ€§",
                    "expected_result": "HTTP 200å“åº”"
                },
                {
                    "scenario": "æ•°æ®æŒä¹…åŒ–æµ‹è¯•", 
                    "description": "éªŒè¯æ•°æ®èƒ½å¤ŸæˆåŠŸå­˜å‚¨åˆ°supermemory",
                    "expected_result": "è¿”å›æœ‰æ•ˆçš„è®°å¿†ID"
                },
                {
                    "scenario": "é”™è¯¯å¤„ç†æµ‹è¯•",
                    "description": "éªŒè¯APIé”™è¯¯çš„æ­£ç¡®å¤„ç†",
                    "expected_result": "ä¼˜é›…çš„é”™è¯¯å¤„ç†"
                }
            ]
            
            test_results = []
            headers = {
                "Authorization": f"Bearer {self.supermemory_api_key}",
                "Content-Type": "application/json"
            }
            
            # æ‰§è¡Œç”Ÿæˆçš„æµ‹è¯•åœºæ™¯
            for i, scenario in enumerate(test_scenarios):
                scenario_start = time.time()
                
                if scenario["scenario"] == "APIè¿é€šæ€§æµ‹è¯•":
                    # æµ‹è¯•GET /memoriesç«¯ç‚¹
                    response = requests.get(f"{self.base_url}/memories", headers=headers, timeout=30)
                    success = response.status_code == 200
                    
                elif scenario["scenario"] == "æ•°æ®æŒä¹…åŒ–æµ‹è¯•":
                    # æµ‹è¯•POST /memoriesç«¯ç‚¹
                    test_data = {
                        "title": f"æ™ºèƒ½æµ‹è¯•ç”Ÿæˆ - åœºæ™¯{i+1}",
                        "content": f"è¿™æ˜¯æ™ºèƒ½æµ‹è¯•ç”Ÿæˆç³»ç»Ÿåˆ›å»ºçš„æµ‹è¯•æ•°æ® - {datetime.now().isoformat()}",
                        "metadata": {"test_scenario": scenario["scenario"]}
                    }
                    response = requests.post(f"{self.base_url}/memories", headers=headers, json=test_data, timeout=30)
                    success = response.status_code in [200, 201]
                    
                elif scenario["scenario"] == "é”™è¯¯å¤„ç†æµ‹è¯•":
                    # æµ‹è¯•æ— æ•ˆè¯·æ±‚çš„é”™è¯¯å¤„ç†
                    invalid_data = {"invalid": "data"}
                    response = requests.post(f"{self.base_url}/memories", headers=headers, json=invalid_data, timeout=30)
                    # é¢„æœŸä¼šå¤±è´¥ï¼Œä½†ç³»ç»Ÿåº”è¯¥ä¼˜é›…å¤„ç†
                    success = response.status_code >= 400  # é”™è¯¯çŠ¶æ€ç è¡¨ç¤ºæ­£ç¡®çš„é”™è¯¯å¤„ç†
                
                scenario_time = time.time() - scenario_start
                
                test_results.append({
                    "scenario": scenario["scenario"],
                    "success": success,
                    "execution_time": scenario_time,
                    "status_code": response.status_code,
                    "response_size": len(response.text)
                })
            
            execution_time = time.time() - start_time
            successful_scenarios = sum(1 for r in test_results if r["success"])
            success_rate = successful_scenarios / len(test_scenarios) * 100
            
            details = {
                "generated_scenarios": len(test_scenarios),
                "executed_scenarios": len(test_results),
                "successful_scenarios": successful_scenarios,
                "success_rate": success_rate,
                "test_results": test_results,
                "average_scenario_time": statistics.mean([r["execution_time"] for r in test_results])
            }
            
            result = TestResult(
                test_name="intelligent_test_generation_api_integration",
                success=success_rate >= 66.7,  # è‡³å°‘2/3çš„åœºæ™¯æˆåŠŸ
                execution_time=execution_time,
                details=details,
                timestamp=datetime.now()
            )
            
            logger.info(f"âœ… æ™ºèƒ½æµ‹è¯•ç”Ÿæˆé›†æˆæµ‹è¯•å®Œæˆ - {execution_time:.3f}s")
            logger.info(f"   æˆåŠŸç‡: {success_rate:.1f}% ({successful_scenarios}/{len(test_scenarios)})")
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            result = TestResult(
                test_name="intelligent_test_generation_api_integration",
                success=False,
                execution_time=execution_time,
                details={"error": error_msg},
                error_message=error_msg,
                timestamp=datetime.now()
            )
            
            logger.error(f"âŒ æ™ºèƒ½æµ‹è¯•ç”Ÿæˆé›†æˆæµ‹è¯•å¤±è´¥: {error_msg}")
            return result
    
    def test_ai_coordination_hub_with_api(self) -> TestResult:
        """æµ‹è¯•AIåè°ƒä¸­æ¢ä¸APIçš„é›†æˆ"""
        logger.info("ğŸ¤– æµ‹è¯•AIåè°ƒä¸­æ¢ä¸APIé›†æˆ")
        start_time = time.time()
        
        try:
            # æ¨¡æ‹ŸAIåè°ƒä¸­æ¢çš„å¤šAIæ¨¡å‹ååŒå·¥ä½œ
            ai_models = [
                {"name": "memory_manager", "role": "æ•°æ®å­˜å‚¨", "api": "supermemory"},
                {"name": "content_analyzer", "role": "å†…å®¹åˆ†æ", "api": "supermemory_search"},
                {"name": "decision_maker", "role": "å†³ç­–åˆ¶å®š", "api": "internal"}
            ]
            
            coordination_tasks = []
            headers = {
                "Authorization": f"Bearer {self.supermemory_api_key}",
                "Content-Type": "application/json"
            }
            
            # ä»»åŠ¡1: å¤šæ¨¡å‹ååŒæ•°æ®å¤„ç†
            task1_start = time.time()
            
            # AIæ¨¡å‹1: å­˜å‚¨åŸå§‹æ•°æ®
            raw_data = {
                "title": "AIåè°ƒä¸­æ¢æµ‹è¯•æ•°æ®",
                "content": "è¿™æ˜¯AIåè°ƒä¸­æ¢å¤šæ¨¡å‹ååŒå¤„ç†çš„æµ‹è¯•æ•°æ®ã€‚åŒ…å«å¤æ‚çš„ä¸šåŠ¡é€»è¾‘å’Œå¤šå±‚æ¬¡çš„æ•°æ®ç»“æ„ã€‚",
                "metadata": {
                    "coordination_test": True,
                    "models_involved": ["memory_manager", "content_analyzer", "decision_maker"],
                    "timestamp": datetime.now().isoformat()
                }
            }
            
            response1 = requests.post(f"{self.base_url}/memories", headers=headers, json=raw_data, timeout=30)
            memory_storage_success = response1.status_code in [200, 201]
            memory_id = response1.json().get('id') if memory_storage_success else None
            
            # AIæ¨¡å‹2: åˆ†æå­˜å‚¨çš„æ•°æ® (ç­‰å¾…å¤„ç†å®Œæˆ)
            time.sleep(2)
            
            # å°è¯•æœç´¢åˆšå­˜å‚¨çš„æ•°æ®
            search_payload = {"q": "AIåè°ƒä¸­æ¢", "limit": 3}
            response2 = requests.post(f"{self.base_url}/search", headers=headers, json=search_payload, timeout=30)
            content_analysis_success = response2.status_code == 200
            
            # AIæ¨¡å‹3: åŸºäºå‰ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœåšå†³ç­–
            decision_result = {
                "storage_success": memory_storage_success,
                "analysis_success": content_analysis_success,
                "coordination_effectiveness": (memory_storage_success and content_analysis_success),
                "recommended_action": "continue" if (memory_storage_success and content_analysis_success) else "retry"
            }
            
            task1_time = time.time() - task1_start
            coordination_tasks.append({
                "task": "multi_model_data_processing",
                "success": decision_result["coordination_effectiveness"],
                "execution_time": task1_time,
                "models_used": 3,
                "api_calls": 2
            })
            
            # ä»»åŠ¡2: é”™è¯¯æ¢å¤åè°ƒ
            task2_start = time.time()
            
            # æ¨¡æ‹Ÿä¸€ä¸ªä¼šå¤±è´¥çš„APIè°ƒç”¨
            invalid_payload = {"invalid": "structure"}
            response3 = requests.post(f"{self.base_url}/memories", headers=headers, json=invalid_payload, timeout=30)
            
            # AIåè°ƒä¸­æ¢æ£€æµ‹åˆ°é”™è¯¯å¹¶å°è¯•æ¢å¤
            if response3.status_code >= 400:
                # é”™è¯¯æ¢å¤: ä½¿ç”¨æ­£ç¡®çš„æ•°æ®æ ¼å¼é‡è¯•
                recovery_data = {
                    "title": "é”™è¯¯æ¢å¤æµ‹è¯•",
                    "content": "AIåè°ƒä¸­æ¢æ£€æµ‹åˆ°é”™è¯¯å¹¶è‡ªåŠ¨æ¢å¤",
                    "metadata": {"recovery_attempt": True}
                }
                response4 = requests.post(f"{self.base_url}/memories", headers=headers, json=recovery_data, timeout=30)
                recovery_success = response4.status_code in [200, 201]
            else:
                recovery_success = False
            
            task2_time = time.time() - task2_start
            coordination_tasks.append({
                "task": "error_recovery_coordination",
                "success": recovery_success,
                "execution_time": task2_time,
                "error_detected": True,
                "recovery_attempted": True
            })
            
            execution_time = time.time() - start_time
            successful_tasks = sum(1 for task in coordination_tasks if task["success"])
            coordination_effectiveness = successful_tasks / len(coordination_tasks) * 100
            
            details = {
                "ai_models": ai_models,
                "coordination_tasks": coordination_tasks,
                "successful_tasks": successful_tasks,
                "total_tasks": len(coordination_tasks),
                "coordination_effectiveness": coordination_effectiveness,
                "total_api_calls": sum(task.get("api_calls", 1) for task in coordination_tasks),
                "average_task_time": statistics.mean([task["execution_time"] for task in coordination_tasks])
            }
            
            result = TestResult(
                test_name="ai_coordination_hub_api_integration",
                success=coordination_effectiveness >= 50,  # è‡³å°‘50%çš„ä»»åŠ¡æˆåŠŸ
                execution_time=execution_time,
                details=details,
                timestamp=datetime.now()
            )
            
            logger.info(f"âœ… AIåè°ƒä¸­æ¢é›†æˆæµ‹è¯•å®Œæˆ - {execution_time:.3f}s")
            logger.info(f"   åè°ƒæ•ˆæœ: {coordination_effectiveness:.1f}% ({successful_tasks}/{len(coordination_tasks)})")
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            result = TestResult(
                test_name="ai_coordination_hub_api_integration",
                success=False,
                execution_time=execution_time,
                details={"error": error_msg},
                error_message=error_msg,
                timestamp=datetime.now()
            )
            
            logger.error(f"âŒ AIåè°ƒä¸­æ¢é›†æˆæµ‹è¯•å¤±è´¥: {error_msg}")
            return result
    
    def test_performance_optimization_with_api(self) -> TestResult:
        """æµ‹è¯•æ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿä¸APIçš„é›†æˆ"""
        logger.info("âš¡ æµ‹è¯•æ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿä¸APIé›†æˆ")
        start_time = time.time()
        
        try:
            # æ¨¡æ‹Ÿæ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿçš„å·¥ä½œ
            optimization_metrics = {
                "api_response_times": [],
                "throughput_measurements": [],
                "error_rates": [],
                "optimization_actions": []
            }
            
            headers = {
                "Authorization": f"Bearer {self.supermemory_api_key}",
                "Content-Type": "application/json"
            }
            
            # æ€§èƒ½åŸºå‡†æµ‹è¯•
            logger.info("   æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
            baseline_times = []
            
            for i in range(5):
                test_data = {
                    "title": f"æ€§èƒ½åŸºå‡†æµ‹è¯• {i+1}",
                    "content": f"æ€§èƒ½ä¼˜åŒ–ç³»ç»ŸåŸºå‡†æµ‹è¯•æ•°æ® - æ‰¹æ¬¡{i+1}",
                    "metadata": {"performance_test": True, "batch": i+1}
                }
                
                call_start = time.time()
                response = requests.post(f"{self.base_url}/memories", headers=headers, json=test_data, timeout=30)
                call_time = time.time() - call_start
                
                baseline_times.append(call_time)
                optimization_metrics["api_response_times"].append(call_time)
                
                if response.status_code not in [200, 201]:
                    optimization_metrics["error_rates"].append(1)
                else:
                    optimization_metrics["error_rates"].append(0)
                
                time.sleep(0.5)  # é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
            
            # åˆ†ææ€§èƒ½æ•°æ®
            avg_baseline = statistics.mean(baseline_times)
            max_baseline = max(baseline_times)
            min_baseline = min(baseline_times)
            
            logger.info(f"   åŸºå‡†æ€§èƒ½: å¹³å‡{avg_baseline:.3f}s, æœ€å¤§{max_baseline:.3f}s, æœ€å°{min_baseline:.3f}s")
            
            # æ¨¡æ‹Ÿæ€§èƒ½ä¼˜åŒ–æªæ–½
            optimization_actions = []
            
            # ä¼˜åŒ–1: è¯·æ±‚æ‰¹å¤„ç†
            if avg_baseline > 0.5:
                optimization_actions.append({
                    "action": "request_batching",
                    "description": "å®æ–½è¯·æ±‚æ‰¹å¤„ç†ä»¥å‡å°‘ç½‘ç»œå¼€é”€",
                    "expected_improvement": "20-30%"
                })
            
            # ä¼˜åŒ–2: è¿æ¥æ± ä¼˜åŒ–
            if max_baseline > 1.0:
                optimization_actions.append({
                    "action": "connection_pooling",
                    "description": "ä¼˜åŒ–HTTPè¿æ¥æ± é…ç½®",
                    "expected_improvement": "15-25%"
                })
            
            # ä¼˜åŒ–3: ç¼“å­˜ç­–ç•¥
            optimization_actions.append({
                "action": "intelligent_caching",
                "description": "å®æ–½æ™ºèƒ½ç¼“å­˜ç­–ç•¥",
                "expected_improvement": "30-50%"
            })
            
            # æ¨¡æ‹Ÿä¼˜åŒ–åçš„æ€§èƒ½æµ‹è¯•
            logger.info("   æ‰§è¡Œä¼˜åŒ–åæ€§èƒ½æµ‹è¯•...")
            optimized_times = []
            
            # ä½¿ç”¨sessionæ¥æ¨¡æ‹Ÿè¿æ¥æ± ä¼˜åŒ–
            session = requests.Session()
            
            for i in range(3):
                test_data = {
                    "title": f"ä¼˜åŒ–åæ€§èƒ½æµ‹è¯• {i+1}",
                    "content": f"æ€§èƒ½ä¼˜åŒ–åçš„æµ‹è¯•æ•°æ® - æ‰¹æ¬¡{i+1}",
                    "metadata": {"optimized_test": True, "batch": i+1}
                }
                
                call_start = time.time()
                response = session.post(f"{self.base_url}/memories", headers=headers, json=test_data, timeout=30)
                call_time = time.time() - call_start
                
                optimized_times.append(call_time)
                time.sleep(0.3)
            
            session.close()
            
            # è®¡ç®—æ€§èƒ½æ”¹è¿›
            avg_optimized = statistics.mean(optimized_times)
            performance_improvement = ((avg_baseline - avg_optimized) / avg_baseline) * 100 if avg_baseline > 0 else 0
            
            execution_time = time.time() - start_time
            
            details = {
                "baseline_performance": {
                    "average_time": avg_baseline,
                    "max_time": max_baseline,
                    "min_time": min_baseline,
                    "measurements": len(baseline_times)
                },
                "optimized_performance": {
                    "average_time": avg_optimized,
                    "measurements": len(optimized_times),
                    "improvement_percentage": performance_improvement
                },
                "optimization_actions": optimization_actions,
                "error_rate": statistics.mean(optimization_metrics["error_rates"]) * 100,
                "total_api_calls": len(baseline_times) + len(optimized_times)
            }
            
            result = TestResult(
                test_name="performance_optimization_api_integration",
                success=performance_improvement >= 0,  # ä»»ä½•æ”¹è¿›éƒ½ç®—æˆåŠŸ
                execution_time=execution_time,
                details=details,
                timestamp=datetime.now()
            )
            
            logger.info(f"âœ… æ€§èƒ½ä¼˜åŒ–é›†æˆæµ‹è¯•å®Œæˆ - {execution_time:.3f}s")
            logger.info(f"   æ€§èƒ½æ”¹è¿›: {performance_improvement:.1f}%")
            logger.info(f"   ä¼˜åŒ–æªæ–½: {len(optimization_actions)}é¡¹")
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            result = TestResult(
                test_name="performance_optimization_api_integration",
                success=False,
                execution_time=execution_time,
                details={"error": error_msg},
                error_message=error_msg,
                timestamp=datetime.now()
            )
            
            logger.error(f"âŒ æ€§èƒ½ä¼˜åŒ–é›†æˆæµ‹è¯•å¤±è´¥: {error_msg}")
            return result
    
    def test_error_handling_and_recovery_with_api(self) -> TestResult:
        """æµ‹è¯•é”™è¯¯å¤„ç†å’Œæ¢å¤ç³»ç»Ÿä¸APIçš„é›†æˆ"""
        logger.info("ğŸ›¡ï¸ æµ‹è¯•é”™è¯¯å¤„ç†å’Œæ¢å¤ç³»ç»Ÿä¸APIé›†æˆ")
        start_time = time.time()
        
        try:
            error_scenarios = []
            headers = {
                "Authorization": f"Bearer {self.supermemory_api_key}",
                "Content-Type": "application/json"
            }
            
            # é”™è¯¯åœºæ™¯1: æ— æ•ˆæ•°æ®æ ¼å¼
            scenario1_start = time.time()
            invalid_data = {"completely": "wrong", "format": 123}
            
            try:
                response1 = requests.post(f"{self.base_url}/memories", headers=headers, json=invalid_data, timeout=30)
                error_detected = response1.status_code >= 400
                
                # é”™è¯¯æ¢å¤: ä½¿ç”¨æ­£ç¡®æ ¼å¼é‡è¯•
                if error_detected:
                    recovery_data = {
                        "title": "é”™è¯¯æ¢å¤æµ‹è¯• - åœºæ™¯1",
                        "content": "æ£€æµ‹åˆ°æ•°æ®æ ¼å¼é”™è¯¯ï¼Œè‡ªåŠ¨ä½¿ç”¨æ­£ç¡®æ ¼å¼é‡è¯•",
                        "metadata": {"recovery_scenario": 1}
                    }
                    recovery_response = requests.post(f"{self.base_url}/memories", headers=headers, json=recovery_data, timeout=30)
                    recovery_success = recovery_response.status_code in [200, 201]
                else:
                    recovery_success = False
                
            except Exception as e:
                error_detected = True
                recovery_success = False
            
            scenario1_time = time.time() - scenario1_start
            error_scenarios.append({
                "scenario": "invalid_data_format",
                "error_detected": error_detected,
                "recovery_attempted": True,
                "recovery_success": recovery_success,
                "execution_time": scenario1_time
            })
            
            # é”™è¯¯åœºæ™¯2: ç½‘ç»œè¶…æ—¶æ¨¡æ‹Ÿ
            scenario2_start = time.time()
            
            try:
                # ä½¿ç”¨å¾ˆçŸ­çš„è¶…æ—¶æ¥æ¨¡æ‹Ÿç½‘ç»œé—®é¢˜
                response2 = requests.post(f"{self.base_url}/memories", headers=headers, json={"title": "è¶…æ—¶æµ‹è¯•"}, timeout=0.001)
                timeout_occurred = False
            except requests.exceptions.Timeout:
                timeout_occurred = True
                
                # é”™è¯¯æ¢å¤: å¢åŠ è¶…æ—¶æ—¶é—´é‡è¯•
                try:
                    recovery_data = {
                        "title": "ç½‘ç»œè¶…æ—¶æ¢å¤æµ‹è¯•",
                        "content": "æ£€æµ‹åˆ°ç½‘ç»œè¶…æ—¶ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´é‡è¯•",
                        "metadata": {"recovery_scenario": 2}
                    }
                    recovery_response = requests.post(f"{self.base_url}/memories", headers=headers, json=recovery_data, timeout=30)
                    recovery_success = recovery_response.status_code in [200, 201]
                except:
                    recovery_success = False
            except Exception:
                timeout_occurred = True
                recovery_success = False
            
            scenario2_time = time.time() - scenario2_start
            error_scenarios.append({
                "scenario": "network_timeout",
                "error_detected": timeout_occurred,
                "recovery_attempted": True,
                "recovery_success": recovery_success,
                "execution_time": scenario2_time
            })
            
            # é”™è¯¯åœºæ™¯3: APIé™æµå¤„ç†
            scenario3_start = time.time()
            
            # å¿«é€Ÿè¿ç»­è¯·æ±‚æ¥è§¦å‘å¯èƒ½çš„é™æµ
            rapid_requests = []
            for i in range(3):
                try:
                    response = requests.post(
                        f"{self.base_url}/memories",
                        headers=headers,
                        json={"title": f"é™æµæµ‹è¯• {i+1}", "content": "å¿«é€Ÿè¯·æ±‚æµ‹è¯•"},
                        timeout=10
                    )
                    rapid_requests.append(response.status_code)
                    time.sleep(0.1)  # å¾ˆçŸ­çš„é—´éš”
                except Exception as e:
                    rapid_requests.append(0)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰429 (Too Many Requests) æˆ–å…¶ä»–é™æµå“åº”
            rate_limit_detected = any(code == 429 for code in rapid_requests)
            
            # å¦‚æœæ£€æµ‹åˆ°é™æµï¼Œå®æ–½é€€é¿é‡è¯•ç­–ç•¥
            if rate_limit_detected:
                time.sleep(2)  # é€€é¿ç­‰å¾…
                recovery_data = {
                    "title": "é™æµæ¢å¤æµ‹è¯•",
                    "content": "æ£€æµ‹åˆ°APIé™æµï¼Œå®æ–½é€€é¿é‡è¯•ç­–ç•¥",
                    "metadata": {"recovery_scenario": 3}
                }
                try:
                    recovery_response = requests.post(f"{self.base_url}/memories", headers=headers, json=recovery_data, timeout=30)
                    recovery_success = recovery_response.status_code in [200, 201]
                except:
                    recovery_success = False
            else:
                recovery_success = True  # æ²¡æœ‰é™æµå°±ç®—æˆåŠŸ
            
            scenario3_time = time.time() - scenario3_start
            error_scenarios.append({
                "scenario": "api_rate_limiting",
                "error_detected": rate_limit_detected,
                "recovery_attempted": rate_limit_detected,
                "recovery_success": recovery_success,
                "execution_time": scenario3_time
            })
            
            execution_time = time.time() - start_time
            
            # è®¡ç®—é”™è¯¯å¤„ç†æ•ˆæœ
            total_scenarios = len(error_scenarios)
            errors_detected = sum(1 for s in error_scenarios if s["error_detected"])
            recoveries_attempted = sum(1 for s in error_scenarios if s["recovery_attempted"])
            recoveries_successful = sum(1 for s in error_scenarios if s["recovery_success"])
            
            error_detection_rate = (errors_detected / total_scenarios) * 100
            recovery_success_rate = (recoveries_successful / recoveries_attempted) * 100 if recoveries_attempted > 0 else 0
            
            details = {
                "error_scenarios": error_scenarios,
                "total_scenarios": total_scenarios,
                "errors_detected": errors_detected,
                "recoveries_attempted": recoveries_attempted,
                "recoveries_successful": recoveries_successful,
                "error_detection_rate": error_detection_rate,
                "recovery_success_rate": recovery_success_rate,
                "average_scenario_time": statistics.mean([s["execution_time"] for s in error_scenarios])
            }
            
            result = TestResult(
                test_name="error_handling_recovery_api_integration",
                success=recovery_success_rate >= 50,  # è‡³å°‘50%çš„æ¢å¤æˆåŠŸ
                execution_time=execution_time,
                details=details,
                timestamp=datetime.now()
            )
            
            logger.info(f"âœ… é”™è¯¯å¤„ç†æ¢å¤é›†æˆæµ‹è¯•å®Œæˆ - {execution_time:.3f}s")
            logger.info(f"   é”™è¯¯æ£€æµ‹ç‡: {error_detection_rate:.1f}%")
            logger.info(f"   æ¢å¤æˆåŠŸç‡: {recovery_success_rate:.1f}%")
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            result = TestResult(
                test_name="error_handling_recovery_api_integration",
                success=False,
                execution_time=execution_time,
                details={"error": error_msg},
                error_message=error_msg,
                timestamp=datetime.now()
            )
            
            logger.error(f"âŒ é”™è¯¯å¤„ç†æ¢å¤é›†æˆæµ‹è¯•å¤±è´¥: {error_msg}")
            return result
    
    def run_complete_test_suite(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡ŒPowerAutomationå®Œæ•´æµ‹è¯•å¥—ä»¶ (çœŸå®APIç‰ˆæœ¬)")
        logger.info("=" * 80)
        
        suite_start_time = time.time()
        
        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        tests = [
            self.test_workflow_engine_with_real_api,
            self.test_intelligent_test_generation_with_api,
            self.test_ai_coordination_hub_with_api,
            self.test_performance_optimization_with_api,
            self.test_error_handling_and_recovery_with_api
        ]
        
        for test_func in tests:
            try:
                result = test_func()
                self.results.append(result)
                self.save_result(result)
                print()  # æ·»åŠ ç©ºè¡Œåˆ†éš”
            except Exception as e:
                logger.error(f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {test_func.__name__} - {str(e)}")
        
        suite_execution_time = time.time() - suite_start_time
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results if r.success)
        success_rate = (successful_tests / total_tests) * 100 if total_tests > 0 else 0
        
        avg_execution_time = statistics.mean([r.execution_time for r in self.results]) if self.results else 0
        total_api_calls = sum(
            r.details.get("total_api_calls", 0) if r.details else 0 
            for r in self.results
        )
        
        comprehensive_report = {
            "test_suite_summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "failed_tests": total_tests - successful_tests,
                "success_rate": success_rate,
                "suite_execution_time": suite_execution_time,
                "average_test_time": avg_execution_time,
                "total_api_calls": total_api_calls,
                "timestamp": datetime.now().isoformat()
            },
            "test_results": [
                {
                    "test_name": r.test_name,
                    "success": r.success,
                    "execution_time": r.execution_time,
                    "error_message": r.error_message,
                    "key_metrics": self._extract_key_metrics(r.details) if r.details else {}
                }
                for r in self.results
            ],
            "performance_analysis": {
                "fastest_test": min(self.results, key=lambda x: x.execution_time).test_name if self.results else None,
                "slowest_test": max(self.results, key=lambda x: x.execution_time).test_name if self.results else None,
                "api_integration_effectiveness": self._calculate_api_effectiveness()
            },
            "detailed_results": [
                {
                    "test_name": r.test_name,
                    "success": r.success,
                    "execution_time": r.execution_time,
                    "details": r.details,
                    "error_message": r.error_message,
                    "timestamp": r.timestamp.isoformat() if r.timestamp else None
                }
                for r in self.results
            ]
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = "/home/ubuntu/powerautomation/complete_test_suite_real_api_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False)
        
        # æ˜¾ç¤ºæ‘˜è¦
        print("ğŸ‰ PowerAutomationå®Œæ•´æµ‹è¯•å¥—ä»¶æ‰§è¡Œå®Œæˆ!")
        print("=" * 60)
        print(f"ğŸ“Š æ€»ä½“ç»“æœ:")
        print(f"   æµ‹è¯•æ€»æ•°: {total_tests}")
        print(f"   æˆåŠŸæµ‹è¯•: {successful_tests}")
        print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"   æ€»æ‰§è¡Œæ—¶é—´: {suite_execution_time:.3f}s")
        print(f"   APIè°ƒç”¨æ€»æ•°: {total_api_calls}")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_path}")
        
        if successful_tests < total_tests:
            print(f"\nâŒ å¤±è´¥çš„æµ‹è¯•:")
            for result in self.results:
                if not result.success:
                    print(f"   â€¢ {result.test_name}: {result.error_message}")
        
        return comprehensive_report
    
    def _extract_key_metrics(self, details: Dict[str, Any]) -> Dict[str, Any]:
        """æå–å…³é”®æŒ‡æ ‡"""
        key_metrics = {}
        
        if "success_rate" in details:
            key_metrics["success_rate"] = details["success_rate"]
        if "coordination_effectiveness" in details:
            key_metrics["coordination_effectiveness"] = details["coordination_effectiveness"]
        if "performance_improvement" in details.get("optimized_performance", {}):
            key_metrics["performance_improvement"] = details["optimized_performance"]["performance_improvement"]
        if "recovery_success_rate" in details:
            key_metrics["recovery_success_rate"] = details["recovery_success_rate"]
        if "total_api_calls" in details:
            key_metrics["api_calls"] = details["total_api_calls"]
            
        return key_metrics
    
    def _calculate_api_effectiveness(self) -> float:
        """è®¡ç®—APIé›†æˆæ•ˆæœ"""
        if not self.results:
            return 0.0
        
        # åŸºäºå„ç§å› ç´ è®¡ç®—APIé›†æˆæ•ˆæœ
        success_weight = 0.4
        performance_weight = 0.3
        error_handling_weight = 0.3
        
        success_score = (sum(1 for r in self.results if r.success) / len(self.results)) * 100
        
        # æ€§èƒ½å¾—åˆ† (åŸºäºæ‰§è¡Œæ—¶é—´)
        avg_time = statistics.mean([r.execution_time for r in self.results])
        performance_score = max(0, 100 - (avg_time * 10))  # æ—¶é—´è¶ŠçŸ­å¾—åˆ†è¶Šé«˜
        
        # é”™è¯¯å¤„ç†å¾—åˆ†
        error_handling_results = [r for r in self.results if "recovery_success_rate" in (r.details or {})]
        if error_handling_results:
            error_handling_score = statistics.mean([
                r.details["recovery_success_rate"] for r in error_handling_results
            ])
        else:
            error_handling_score = 50  # é»˜è®¤åˆ†æ•°
        
        effectiveness = (
            success_score * success_weight +
            performance_score * performance_weight +
            error_handling_score * error_handling_weight
        )
        
        return round(effectiveness, 2)

def main():
    """ä¸»å‡½æ•°"""
    test_suite = RealAPIIntegratedTestSuite()
    report = test_suite.run_complete_test_suite()
    return report

if __name__ == "__main__":
    main()

