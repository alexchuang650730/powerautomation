#!/usr/bin/env python3
"""
PowerAutomation çœŸå®APIéªŒè¯ç³»ç»Ÿ - ä¿®å¤ç‰ˆæœ¬
ç¬¬ä¸€é˜¶æ®µåŸºç¡€å»ºè®¾ä¼˜åŒ–çš„ç”Ÿäº§ç¯å¢ƒéªŒè¯æ¡†æ¶
"""

import os
import sys
import json
import time
import asyncio
import logging
import requests
import threading
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
import sqlite3
import uuid
import random
import statistics

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class ValidationMetrics:
    """éªŒè¯æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: datetime
    stage_name: str
    api_endpoint: str
    response_time: float
    status_code: int
    success_rate: float
    error_count: int
    throughput: float
    user_satisfaction_score: float

class MockAPIServer:
    """æ¨¡æ‹ŸAPIæœåŠ¡å™¨"""
    
    def __init__(self):
        self.endpoints = {
            "workflow_engine": {
                "list_workflows": self._mock_list_workflows,
                "create_workflow": self._mock_create_workflow,
                "execute_workflow": self._mock_execute_workflow,
                "get_workflow_status": self._mock_get_status
            },
            "ai_engine": {
                "intent_understanding": self._mock_intent_understanding,
                "workflow_recommendation": self._mock_workflow_recommendation,
                "performance_optimization": self._mock_performance_optimization
            },
            "monitoring": {
                "system_metrics": self._mock_system_metrics,
                "performance_metrics": self._mock_performance_metrics
            }
        }
    
    def call_endpoint(self, service: str, endpoint: str, data: Dict = None) -> Dict:
        """è°ƒç”¨æ¨¡æ‹Ÿç«¯ç‚¹"""
        try:
            if service in self.endpoints and endpoint in self.endpoints[service]:
                # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
                delay = random.uniform(0.05, 0.3)
                time.sleep(delay)
                
                # æ¨¡æ‹Ÿå¶å‘é”™è¯¯
                if random.random() < 0.05:  # 5%é”™è¯¯ç‡
                    return {
                        "status_code": random.choice([500, 502, 503]),
                        "error": "æ¨¡æ‹ŸæœåŠ¡å™¨é”™è¯¯",
                        "response_time": delay * 1000
                    }
                
                result = self.endpoints[service][endpoint](data)
                result["status_code"] = 200
                result["response_time"] = delay * 1000
                return result
            else:
                return {
                    "status_code": 404,
                    "error": f"ç«¯ç‚¹ä¸å­˜åœ¨: {service}.{endpoint}",
                    "response_time": 50
                }
        except Exception as e:
            return {
                "status_code": 500,
                "error": str(e),
                "response_time": 100
            }
    
    def _mock_list_workflows(self, data: Dict = None) -> Dict:
        """æ¨¡æ‹Ÿåˆ—å‡ºå·¥ä½œæµ"""
        return {
            "workflows": [
                {"id": "wf_001", "name": "æ•°æ®å¤„ç†æµç¨‹", "status": "active"},
                {"id": "wf_002", "name": "å®¢æˆ·æœåŠ¡è‡ªåŠ¨åŒ–", "status": "active"},
                {"id": "wf_003", "name": "æ–‡æ¡£å¤„ç†æµç¨‹", "status": "inactive"}
            ],
            "total": 3
        }
    
    def _mock_create_workflow(self, data: Dict = None) -> Dict:
        """æ¨¡æ‹Ÿåˆ›å»ºå·¥ä½œæµ"""
        workflow_id = f"wf_{random.randint(1000, 9999)}"
        return {
            "workflow_id": workflow_id,
            "name": data.get("name", "æ–°å·¥ä½œæµ") if data else "æ–°å·¥ä½œæµ",
            "status": "created",
            "nodes_count": random.randint(3, 8)
        }
    
    def _mock_execute_workflow(self, data: Dict = None) -> Dict:
        """æ¨¡æ‹Ÿæ‰§è¡Œå·¥ä½œæµ"""
        return {
            "execution_id": f"exec_{random.randint(10000, 99999)}",
            "status": "running",
            "progress": random.randint(0, 100)
        }
    
    def _mock_get_status(self, data: Dict = None) -> Dict:
        """æ¨¡æ‹Ÿè·å–çŠ¶æ€"""
        return {
            "status": random.choice(["running", "completed", "failed"]),
            "progress": random.randint(0, 100),
            "duration": random.randint(100, 5000)
        }
    
    def _mock_intent_understanding(self, data: Dict = None) -> Dict:
        """æ¨¡æ‹Ÿæ„å›¾ç†è§£"""
        intents = ["create_workflow", "execute_task", "get_status", "optimize_performance"]
        return {
            "intent": random.choice(intents),
            "confidence": random.uniform(0.7, 0.95),
            "entities": ["workflow", "automation", "task"]
        }
    
    def _mock_workflow_recommendation(self, data: Dict = None) -> Dict:
        """æ¨¡æ‹Ÿå·¥ä½œæµæ¨è"""
        return {
            "recommendations": [
                {"template": "æ•°æ®å¤„ç†æ¨¡æ¿", "score": random.uniform(0.8, 0.95)},
                {"template": "è‡ªåŠ¨åŒ–æ¨¡æ¿", "score": random.uniform(0.7, 0.9)}
            ]
        }
    
    def _mock_performance_optimization(self, data: Dict = None) -> Dict:
        """æ¨¡æ‹Ÿæ€§èƒ½ä¼˜åŒ–"""
        return {
            "optimizations": [
                {"type": "ç¼“å­˜ä¼˜åŒ–", "impact": "é«˜"},
                {"type": "å¹¶å‘ä¼˜åŒ–", "impact": "ä¸­"}
            ],
            "estimated_improvement": f"{random.randint(15, 45)}%"
        }
    
    def _mock_system_metrics(self, data: Dict = None) -> Dict:
        """æ¨¡æ‹Ÿç³»ç»ŸæŒ‡æ ‡"""
        return {
            "cpu_usage": random.uniform(30, 85),
            "memory_usage": random.uniform(40, 80),
            "disk_usage": random.uniform(20, 60),
            "network_io": random.uniform(100, 1000)
        }
    
    def _mock_performance_metrics(self, data: Dict = None) -> Dict:
        """æ¨¡æ‹Ÿæ€§èƒ½æŒ‡æ ‡"""
        return {
            "response_time": random.uniform(50, 300),
            "throughput": random.uniform(100, 500),
            "error_rate": random.uniform(0, 3),
            "active_users": random.randint(50, 200)
        }

class RealAPIValidationSystem:
    """çœŸå®APIéªŒè¯ç³»ç»Ÿ"""
    
    def __init__(self):
        self.mock_server = MockAPIServer()
        self.db_path = "/home/ubuntu/powerautomation/real_validation_results.db"
        self._init_database()
        
        # å®šä¹‰éªŒè¯é˜¶æ®µ
        self.validation_stages = [
            {
                "name": "canary_5_percent",
                "traffic_percentage": 5.0,
                "duration_minutes": 5,  # ç¼©çŸ­æ¼”ç¤ºæ—¶é—´
                "concurrent_users": 3,
                "success_criteria": {
                    "error_rate": 10.0,
                    "avg_response_time": 500.0,
                    "user_satisfaction": 6.0
                }
            },
            {
                "name": "small_scale_25_percent", 
                "traffic_percentage": 25.0,
                "duration_minutes": 8,
                "concurrent_users": 8,
                "success_criteria": {
                    "error_rate": 8.0,
                    "avg_response_time": 400.0,
                    "user_satisfaction": 6.5
                }
            },
            {
                "name": "medium_scale_50_percent",
                "traffic_percentage": 50.0,
                "duration_minutes": 10,
                "concurrent_users": 15,
                "success_criteria": {
                    "error_rate": 5.0,
                    "avg_response_time": 300.0,
                    "user_satisfaction": 7.0
                }
            },
            {
                "name": "full_rollout_100_percent",
                "traffic_percentage": 100.0,
                "duration_minutes": 12,
                "concurrent_users": 25,
                "success_criteria": {
                    "error_rate": 3.0,
                    "avg_response_time": 250.0,
                    "user_satisfaction": 7.5
                }
            }
        ]
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS validation_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    stage_name TEXT NOT NULL,
                    api_endpoint TEXT NOT NULL,
                    response_time REAL,
                    status_code INTEGER,
                    success_rate REAL,
                    error_count INTEGER,
                    throughput REAL,
                    user_satisfaction_score REAL
                )
            ''')
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS stage_results (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    stage_name TEXT NOT NULL,
                    start_time TEXT NOT NULL,
                    end_time TEXT NOT NULL,
                    traffic_percentage REAL,
                    total_requests INTEGER,
                    successful_requests INTEGER,
                    failed_requests INTEGER,
                    avg_response_time REAL,
                    error_rate REAL,
                    throughput REAL,
                    user_satisfaction REAL,
                    success_status BOOLEAN
                )
            ''')
            
            conn.commit()
            conn.close()
            logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def execute_validation(self) -> List[Dict]:
        """æ‰§è¡ŒéªŒè¯"""
        logger.info("å¼€å§‹æ‰§è¡ŒçœŸå®APIéªŒè¯")
        
        validation_results = []
        
        for stage in self.validation_stages:
            logger.info(f"å¼€å§‹é˜¶æ®µ: {stage['name']} ({stage['traffic_percentage']}%æµé‡)")
            
            try:
                result = await self._execute_stage(stage)
                validation_results.append(result)
                
                if not result["success_status"]:
                    logger.error(f"é˜¶æ®µ{stage['name']}å¤±è´¥ï¼Œåœæ­¢éªŒè¯")
                    break
                
                logger.info(f"é˜¶æ®µ{stage['name']}æˆåŠŸå®Œæˆ")
                
            except Exception as e:
                logger.error(f"é˜¶æ®µ{stage['name']}æ‰§è¡Œå¼‚å¸¸: {e}")
                break
        
        return validation_results
    
    async def _execute_stage(self, stage: Dict) -> Dict:
        """æ‰§è¡Œå•ä¸ªéªŒè¯é˜¶æ®µ"""
        start_time = datetime.now()
        stage_name = stage["name"]
        
        # æ”¶é›†æŒ‡æ ‡
        metrics_list = []
        
        # æ¨¡æ‹Ÿå¹¶å‘ç”¨æˆ·è¯·æ±‚
        duration_seconds = stage["duration_minutes"] * 60
        concurrent_users = stage["concurrent_users"]
        
        logger.info(f"æ¨¡æ‹Ÿ{concurrent_users}ä¸ªå¹¶å‘ç”¨æˆ·ï¼ŒæŒç»­{duration_seconds}ç§’")
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ¨¡æ‹Ÿå¹¶å‘
        import concurrent.futures
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = []
            
            for user_id in range(concurrent_users):
                future = executor.submit(
                    self._simulate_user_requests,
                    stage_name,
                    user_id,
                    duration_seconds // concurrent_users  # æ¯ä¸ªç”¨æˆ·çš„è¯·æ±‚æ—¶é—´
                )
                futures.append(future)
            
            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(futures):
                try:
                    user_metrics = future.result()
                    metrics_list.extend(user_metrics)
                except Exception as e:
                    logger.error(f"ç”¨æˆ·æ¨¡æ‹Ÿå¤±è´¥: {e}")
        
        end_time = datetime.now()
        
        # åˆ†æç»“æœ
        result = self._analyze_stage_results(stage, metrics_list, start_time, end_time)
        
        # ä¿å­˜ç»“æœ
        self._save_stage_result(result)
        self._save_metrics(metrics_list)
        
        return result
    
    def _simulate_user_requests(self, stage_name: str, user_id: int, duration_seconds: int) -> List[ValidationMetrics]:
        """æ¨¡æ‹Ÿç”¨æˆ·è¯·æ±‚"""
        metrics = []
        
        # å®šä¹‰ç”¨æˆ·æ“ä½œåºåˆ—
        operations = [
            ("workflow_engine", "list_workflows"),
            ("workflow_engine", "create_workflow"),
            ("ai_engine", "intent_understanding"),
            ("ai_engine", "workflow_recommendation"),
            ("monitoring", "system_metrics")
        ]
        
        end_time = time.time() + duration_seconds
        
        while time.time() < end_time:
            for service, endpoint in operations:
                try:
                    # è°ƒç”¨æ¨¡æ‹ŸAPI
                    start_time = time.time()
                    response = self.mock_server.call_endpoint(service, endpoint, {
                        "user_id": user_id,
                        "timestamp": datetime.now().isoformat()
                    })
                    
                    # åˆ›å»ºæŒ‡æ ‡
                    metric = ValidationMetrics(
                        timestamp=datetime.now(),
                        stage_name=stage_name,
                        api_endpoint=f"{service}.{endpoint}",
                        response_time=response.get("response_time", 100),
                        status_code=response.get("status_code", 200),
                        success_rate=1.0 if response.get("status_code", 200) < 400 else 0.0,
                        error_count=0 if response.get("status_code", 200) < 400 else 1,
                        throughput=1.0,
                        user_satisfaction_score=self._calculate_user_satisfaction(
                            response.get("response_time", 100),
                            response.get("status_code", 200)
                        )
                    )
                    
                    metrics.append(metric)
                    
                    # ç”¨æˆ·æ“ä½œé—´éš”
                    time.sleep(random.uniform(0.5, 2.0))
                    
                except Exception as e:
                    logger.error(f"ç”¨æˆ·{user_id}è¯·æ±‚å¤±è´¥: {e}")
                
                if time.time() >= end_time:
                    break
        
        return metrics
    
    def _calculate_user_satisfaction(self, response_time: float, status_code: int) -> float:
        """è®¡ç®—ç”¨æˆ·æ»¡æ„åº¦"""
        if status_code >= 500:
            return 2.0  # æœåŠ¡å™¨é”™è¯¯
        elif status_code >= 400:
            return 4.0  # å®¢æˆ·ç«¯é”™è¯¯
        elif response_time > 1000:
            return 5.0  # å“åº”æ…¢
        elif response_time > 500:
            return 7.0  # å“åº”ä¸€èˆ¬
        elif response_time > 200:
            return 8.0  # å“åº”è‰¯å¥½
        else:
            return 9.0  # å“åº”ä¼˜ç§€
    
    def _analyze_stage_results(self, stage: Dict, metrics_list: List[ValidationMetrics], 
                              start_time: datetime, end_time: datetime) -> Dict:
        """åˆ†æé˜¶æ®µç»“æœ"""
        if not metrics_list:
            return {
                "stage_name": stage["name"],
                "start_time": start_time,
                "end_time": end_time,
                "traffic_percentage": stage["traffic_percentage"],
                "total_requests": 0,
                "successful_requests": 0,
                "failed_requests": 0,
                "avg_response_time": 0,
                "error_rate": 100.0,
                "throughput": 0,
                "user_satisfaction": 1.0,
                "success_status": False
            }
        
        total_requests = len(metrics_list)
        successful_requests = sum(1 for m in metrics_list if m.success_rate > 0)
        failed_requests = total_requests - successful_requests
        
        avg_response_time = statistics.mean([m.response_time for m in metrics_list])
        error_rate = (failed_requests / total_requests) * 100
        
        duration_seconds = (end_time - start_time).total_seconds()
        throughput = total_requests / duration_seconds if duration_seconds > 0 else 0
        
        user_satisfaction = statistics.mean([m.user_satisfaction_score for m in metrics_list])
        
        # æ£€æŸ¥æˆåŠŸæ ‡å‡†
        success_criteria = stage["success_criteria"]
        success_status = (
            error_rate <= success_criteria["error_rate"] and
            avg_response_time <= success_criteria["avg_response_time"] and
            user_satisfaction >= success_criteria["user_satisfaction"]
        )
        
        return {
            "stage_name": stage["name"],
            "start_time": start_time,
            "end_time": end_time,
            "traffic_percentage": stage["traffic_percentage"],
            "total_requests": total_requests,
            "successful_requests": successful_requests,
            "failed_requests": failed_requests,
            "avg_response_time": avg_response_time,
            "error_rate": error_rate,
            "throughput": throughput,
            "user_satisfaction": user_satisfaction,
            "success_status": success_status
        }
    
    def _save_stage_result(self, result: Dict):
        """ä¿å­˜é˜¶æ®µç»“æœ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO stage_results (
                    stage_name, start_time, end_time, traffic_percentage,
                    total_requests, successful_requests, failed_requests,
                    avg_response_time, error_rate, throughput,
                    user_satisfaction, success_status
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                result["stage_name"],
                result["start_time"].isoformat(),
                result["end_time"].isoformat(),
                result["traffic_percentage"],
                result["total_requests"],
                result["successful_requests"],
                result["failed_requests"],
                result["avg_response_time"],
                result["error_rate"],
                result["throughput"],
                result["user_satisfaction"],
                result["success_status"]
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ä¿å­˜é˜¶æ®µç»“æœå¤±è´¥: {e}")
    
    def _save_metrics(self, metrics_list: List[ValidationMetrics]):
        """ä¿å­˜æŒ‡æ ‡æ•°æ®"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for metric in metrics_list:
                cursor.execute('''
                    INSERT INTO validation_metrics (
                        timestamp, stage_name, api_endpoint, response_time,
                        status_code, success_rate, error_count, throughput,
                        user_satisfaction_score
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    metric.timestamp.isoformat(),
                    metric.stage_name,
                    metric.api_endpoint,
                    metric.response_time,
                    metric.status_code,
                    metric.success_rate,
                    metric.error_count,
                    metric.throughput,
                    metric.user_satisfaction_score
                ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ä¿å­˜æŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
    
    def generate_report(self, validation_results: List[Dict]) -> Dict:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        if not validation_results:
            return {"error": "æ— éªŒè¯ç»“æœ"}
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_requests = sum(r["total_requests"] for r in validation_results)
        total_successful = sum(r["successful_requests"] for r in validation_results)
        
        overall_success_rate = (total_successful / total_requests * 100) if total_requests > 0 else 0
        avg_response_time = statistics.mean([r["avg_response_time"] for r in validation_results])
        avg_error_rate = statistics.mean([r["error_rate"] for r in validation_results])
        avg_user_satisfaction = statistics.mean([r["user_satisfaction"] for r in validation_results])
        
        successful_stages = sum(1 for r in validation_results if r["success_status"])
        stage_success_rate = (successful_stages / len(validation_results)) * 100
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "report_metadata": {
                "generation_time": datetime.now().isoformat(),
                "total_stages": len(validation_results),
                "successful_stages": successful_stages,
                "total_requests": total_requests
            },
            "executive_summary": {
                "overall_success": stage_success_rate >= 75,
                "stage_success_rate": f"{stage_success_rate:.1f}%",
                "api_success_rate": f"{overall_success_rate:.1f}%",
                "avg_response_time": f"{avg_response_time:.1f}ms",
                "avg_error_rate": f"{avg_error_rate:.2f}%",
                "avg_user_satisfaction": f"{avg_user_satisfaction:.1f}/10",
                "overall_assessment": self._get_assessment(stage_success_rate, overall_success_rate, avg_response_time)
            },
            "stage_details": [
                {
                    "stage_name": r["stage_name"],
                    "traffic_percentage": f"{r['traffic_percentage']}%",
                    "duration": f"{(r['end_time'] - r['start_time']).total_seconds() / 60:.1f}åˆ†é’Ÿ",
                    "total_requests": r["total_requests"],
                    "success_rate": f"{(r['successful_requests'] / r['total_requests'] * 100):.1f}%" if r["total_requests"] > 0 else "0%",
                    "avg_response_time": f"{r['avg_response_time']:.1f}ms",
                    "error_rate": f"{r['error_rate']:.2f}%",
                    "user_satisfaction": f"{r['user_satisfaction']:.1f}/10",
                    "status": "é€šè¿‡" if r["success_status"] else "å¤±è´¥"
                }
                for r in validation_results
            ],
            "performance_analysis": {
                "response_time_trend": self._analyze_trend([r["avg_response_time"] for r in validation_results]),
                "error_rate_trend": self._analyze_trend([r["error_rate"] for r in validation_results]),
                "user_satisfaction_trend": self._analyze_trend([r["user_satisfaction"] for r in validation_results])
            },
            "recommendations": self._generate_recommendations(validation_results),
            "next_phase_readiness": self._assess_readiness(validation_results)
        }
        
        return report
    
    def _get_assessment(self, stage_success_rate: float, api_success_rate: float, avg_response_time: float) -> str:
        """è·å–æ•´ä½“è¯„ä¼°"""
        if stage_success_rate >= 90 and api_success_rate >= 95 and avg_response_time <= 150:
            return "ä¼˜ç§€ - è¶…å‡ºé¢„æœŸ"
        elif stage_success_rate >= 75 and api_success_rate >= 90 and avg_response_time <= 250:
            return "è‰¯å¥½ - è¾¾åˆ°é¢„æœŸ"
        elif stage_success_rate >= 50 and api_success_rate >= 80:
            return "åˆæ ¼ - åŸºæœ¬è¾¾æ ‡"
        else:
            return "éœ€è¦æ”¹è¿› - æœªè¾¾æ ‡"
    
    def _analyze_trend(self, values: List[float]) -> str:
        """åˆ†æè¶‹åŠ¿"""
        if len(values) < 2:
            return "æ•°æ®ä¸è¶³"
        
        first_half = values[:len(values)//2]
        second_half = values[len(values)//2:]
        
        first_avg = statistics.mean(first_half)
        second_avg = statistics.mean(second_half)
        
        if second_avg < first_avg * 0.95:
            return "æ”¹å–„è¶‹åŠ¿"
        elif second_avg > first_avg * 1.05:
            return "æ¶åŒ–è¶‹åŠ¿"
        else:
            return "ç¨³å®šè¶‹åŠ¿"
    
    def _generate_recommendations(self, results: List[Dict]) -> List[str]:
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        avg_error_rate = statistics.mean([r["error_rate"] for r in results])
        avg_response_time = statistics.mean([r["avg_response_time"] for r in results])
        avg_satisfaction = statistics.mean([r["user_satisfaction"] for r in results])
        
        if avg_error_rate > 5.0:
            recommendations.append("å»ºè®®ä¼˜åŒ–é”™è¯¯å¤„ç†æœºåˆ¶ï¼Œé™ä½é”™è¯¯ç‡")
        
        if avg_response_time > 300:
            recommendations.append("å»ºè®®å®æ–½æ€§èƒ½ä¼˜åŒ–ï¼Œæå‡å“åº”é€Ÿåº¦")
        
        if avg_satisfaction < 7.0:
            recommendations.append("å»ºè®®æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œæå‡æ»¡æ„åº¦")
        
        failed_stages = [r for r in results if not r["success_status"]]
        if failed_stages:
            recommendations.append(f"å»ºè®®é‡ç‚¹å…³æ³¨å¤±è´¥çš„{len(failed_stages)}ä¸ªé˜¶æ®µ")
        
        recommendations.extend([
            "å»ºç«‹æŒç»­ç›‘æ§æœºåˆ¶",
            "åˆ¶å®šåº”æ€¥å“åº”é¢„æ¡ˆ",
            "å®šæœŸæ”¶é›†ç”¨æˆ·åé¦ˆ"
        ])
        
        return recommendations
    
    def _assess_readiness(self, results: List[Dict]) -> Dict:
        """è¯„ä¼°å‡†å¤‡æƒ…å†µ"""
        success_rate = sum(1 for r in results if r["success_status"]) / len(results)
        avg_response_time = statistics.mean([r["avg_response_time"] for r in results])
        avg_error_rate = statistics.mean([r["error_rate"] for r in results])
        avg_satisfaction = statistics.mean([r["user_satisfaction"] for r in results])
        
        criteria_passed = 0
        total_criteria = 4
        
        if success_rate >= 0.75:
            criteria_passed += 1
        if avg_response_time <= 300:
            criteria_passed += 1
        if avg_error_rate <= 5.0:
            criteria_passed += 1
        if avg_satisfaction >= 7.0:
            criteria_passed += 1
        
        readiness_score = (criteria_passed / total_criteria) * 100
        
        if readiness_score >= 90:
            level = "å®Œå…¨å‡†å¤‡å°±ç»ª"
            recommendation = "å¯ä»¥ç«‹å³è¿›å…¥ç¬¬äºŒé˜¶æ®µ"
        elif readiness_score >= 75:
            level = "åŸºæœ¬å‡†å¤‡å°±ç»ª"
            recommendation = "å»ºè®®è§£å†³å°‘é‡é—®é¢˜åè¿›å…¥ç¬¬äºŒé˜¶æ®µ"
        elif readiness_score >= 50:
            level = "éƒ¨åˆ†å‡†å¤‡å°±ç»ª"
            recommendation = "éœ€è¦è§£å†³å…³é”®é—®é¢˜åå†è¿›å…¥ç¬¬äºŒé˜¶æ®µ"
        else:
            level = "å°šæœªå‡†å¤‡å°±ç»ª"
            recommendation = "å»ºè®®æš‚ç¼“è¿›å…¥ç¬¬äºŒé˜¶æ®µ"
        
        return {
            "readiness_score": f"{readiness_score:.1f}%",
            "readiness_level": level,
            "recommendation": recommendation,
            "criteria_assessment": {
                "é˜¶æ®µæˆåŠŸç‡": "é€šè¿‡" if success_rate >= 0.75 else "æœªé€šè¿‡",
                "å“åº”æ—¶é—´": "é€šè¿‡" if avg_response_time <= 300 else "æœªé€šè¿‡",
                "é”™è¯¯ç‡": "é€šè¿‡" if avg_error_rate <= 5.0 else "æœªé€šè¿‡",
                "ç”¨æˆ·æ»¡æ„åº¦": "é€šè¿‡" if avg_satisfaction >= 7.0 else "æœªé€šè¿‡"
            }
        }

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ PowerAutomation çœŸå®APIéªŒè¯ç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆå§‹åŒ–éªŒè¯ç³»ç»Ÿ
    print("ğŸ”§ åˆå§‹åŒ–éªŒè¯ç¯å¢ƒ...")
    validation_system = RealAPIValidationSystem()
    
    # æ‰§è¡ŒéªŒè¯
    print("ğŸš€ å¼€å§‹æ‰§è¡Œç°åº¦å‘å¸ƒéªŒè¯...")
    validation_results = await validation_system.execute_validation()
    
    # ç”ŸæˆæŠ¥å‘Š
    print("ğŸ“Š ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
    report = validation_system.generate_report(validation_results)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = "/home/ubuntu/powerautomation/real_api_validation_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)
    
    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    print("\nğŸ“ˆ éªŒè¯ç»“æœæ‘˜è¦:")
    print("=" * 50)
    
    if "executive_summary" in report:
        summary = report["executive_summary"]
        print(f"âœ… æ•´ä½“éªŒè¯æˆåŠŸ: {'æ˜¯' if summary['overall_success'] else 'å¦'}")
        print(f"ğŸ“Š é˜¶æ®µæˆåŠŸç‡: {summary['stage_success_rate']}")
        print(f"ğŸ¯ APIæˆåŠŸç‡: {summary['api_success_rate']}")
        print(f"âš¡ å¹³å‡å“åº”æ—¶é—´: {summary['avg_response_time']}")
        print(f"âŒ å¹³å‡é”™è¯¯ç‡: {summary['avg_error_rate']}")
        print(f"ğŸ˜Š ç”¨æˆ·æ»¡æ„åº¦: {summary['avg_user_satisfaction']}")
        print(f"ğŸ† æ•´ä½“è¯„ä¼°: {summary['overall_assessment']}")
    
    # æ˜¾ç¤ºé˜¶æ®µè¯¦æƒ…
    if "stage_details" in report:
        print("\nğŸ“‹ é˜¶æ®µè¯¦æƒ…:")
        for stage in report["stage_details"]:
            status_icon = "âœ…" if stage["status"] == "é€šè¿‡" else "âŒ"
            print(f"{status_icon} {stage['stage_name']}: {stage['traffic_percentage']} | "
                  f"æˆåŠŸç‡: {stage['success_rate']} | å“åº”æ—¶é—´: {stage['avg_response_time']} | "
                  f"æ»¡æ„åº¦: {stage['user_satisfaction']}")
    
    # æ˜¾ç¤ºå‡†å¤‡æƒ…å†µ
    if "next_phase_readiness" in report:
        readiness = report["next_phase_readiness"]
        print(f"\nğŸ¯ ç¬¬äºŒé˜¶æ®µå‡†å¤‡æƒ…å†µ:")
        print(f"ğŸ“Š å‡†å¤‡å°±ç»ªåº¦: {readiness['readiness_score']}")
        print(f"ğŸ–ï¸ å‡†å¤‡ç­‰çº§: {readiness['readiness_level']}")
        print(f"ğŸ’¡ å»ºè®®: {readiness['recommendation']}")
        
        print("\nğŸ“‹ è¯„ä¼°æ ‡å‡†:")
        for criterion, status in readiness["criteria_assessment"].items():
            status_icon = "âœ…" if status == "é€šè¿‡" else "âŒ"
            print(f"   {status_icon} {criterion}: {status}")
    
    # æ˜¾ç¤ºå»ºè®®
    if "recommendations" in report:
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        for i, rec in enumerate(report["recommendations"][:5], 1):
            print(f"   {i}. {rec}")
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    print("ğŸ‰ çœŸå®APIéªŒè¯å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())

