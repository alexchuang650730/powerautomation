#!/usr/bin/env python3
"""
PowerAutomation AIå¢å¼ºåŠŸèƒ½å…¨é¢æµ‹è¯•
ä¸“æ³¨äºAIåŠŸèƒ½çš„æ·±åº¦éªŒè¯å’Œæ€§èƒ½è¯„ä¼°
"""

import sys
import os
import json
import time
import logging
import requests
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import sqlite3
import threading
from concurrent.futures import ThreadPoolExecutor
import statistics
import random

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/ubuntu/powerautomation')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class AITestResult:
    """AIæµ‹è¯•ç»“æœæ•°æ®ç±»"""
    test_name: str
    ai_component: str
    success: bool
    execution_time: float
    ai_metrics: Dict[str, Any]
    performance_score: float
    error_message: Optional[str] = None
    timestamp: datetime = None

class AIEnhancedFunctionTester:
    """AIå¢å¼ºåŠŸèƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.supermemory_api_key = ""SUPERMEMORY_API_KEY_PLACEHOLDER""
        self.base_url = "https://api.supermemory.ai/v3"
        self.results: List[AITestResult] = []
        self.db_path = "/home/ubuntu/powerautomation/ai_enhanced_test_results.db"
        self.init_database()
        
    def init_database(self):
        """åˆå§‹åŒ–AIæµ‹è¯•ç»“æœæ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ai_enhanced_test_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                test_name TEXT NOT NULL,
                ai_component TEXT NOT NULL,
                success BOOLEAN NOT NULL,
                execution_time REAL NOT NULL,
                ai_metrics TEXT,
                performance_score REAL,
                error_message TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info("AIæµ‹è¯•æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    
    def save_result(self, result: AITestResult):
        """ä¿å­˜AIæµ‹è¯•ç»“æœåˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO ai_enhanced_test_results 
            (test_name, ai_component, success, execution_time, ai_metrics, performance_score, error_message, timestamp)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            result.test_name,
            result.ai_component,
            result.success,
            result.execution_time,
            json.dumps(result.ai_metrics) if result.ai_metrics else None,
            result.performance_score,
            result.error_message,
            result.timestamp
        ))
        
        conn.commit()
        conn.close()
    
    def test_ai_intent_understanding(self) -> AITestResult:
        """æµ‹è¯•AIæ„å›¾ç†è§£åŠŸèƒ½"""
        logger.info("ğŸ§  æµ‹è¯•AIæ„å›¾ç†è§£åŠŸèƒ½")
        start_time = time.time()
        
        try:
            # æ¨¡æ‹ŸAIæ„å›¾ç†è§£ç³»ç»Ÿ
            test_intents = [
                {
                    "user_input": "æˆ‘æƒ³ä¿å­˜è¿™ä¸ªé‡è¦çš„ä¼šè®®è®°å½•",
                    "expected_intent": "save_memory",
                    "expected_confidence": 0.9
                },
                {
                    "user_input": "å¸®æˆ‘æ‰¾ä¸€ä¸‹ä¸Šæ¬¡å…³äºé¡¹ç›®çš„è®¨è®º",
                    "expected_intent": "search_memory",
                    "expected_confidence": 0.85
                },
                {
                    "user_input": "åˆ é™¤é‚£ä¸ªè¿‡æ—¶çš„ä¿¡æ¯",
                    "expected_intent": "delete_memory",
                    "expected_confidence": 0.8
                },
                {
                    "user_input": "è¿™ä¸ªæ•°æ®å¾ˆé‡è¦ï¼Œéœ€è¦é•¿æœŸä¿å­˜",
                    "expected_intent": "save_memory",
                    "expected_confidence": 0.88
                },
                {
                    "user_input": "æŸ¥çœ‹æˆ‘çš„æ‰€æœ‰è®°å¿†",
                    "expected_intent": "list_memories",
                    "expected_confidence": 0.95
                }
            ]
            
            intent_results = []
            headers = {
                "Authorization": f"Bearer {self.supermemory_api_key}",
                "Content-Type": "application/json"
            }
            
            for intent_test in test_intents:
                intent_start = time.time()
                
                # æ¨¡æ‹ŸAIæ„å›¾ç†è§£å¤„ç†
                user_input = intent_test["user_input"]
                
                # åŸºäºå…³é”®è¯çš„ç®€å•æ„å›¾è¯†åˆ«ï¼ˆæ¨¡æ‹ŸAIå¤„ç†ï¼‰
                if "ä¿å­˜" in user_input or "å­˜å‚¨" in user_input or "è®°å½•" in user_input:
                    detected_intent = "save_memory"
                    confidence = 0.9 + random.uniform(-0.1, 0.1)
                    
                    # æ‰§è¡Œå¯¹åº”çš„APIæ“ä½œ
                    memory_data = {
                        "title": f"AIæ„å›¾ç†è§£æµ‹è¯• - {user_input[:20]}",
                        "content": f"ç”¨æˆ·æ„å›¾: {user_input}",
                        "metadata": {
                            "detected_intent": detected_intent,
                            "confidence": confidence,
                            "test_type": "intent_understanding"
                        }
                    }
                    
                    response = requests.post(f"{self.base_url}/memories", headers=headers, json=memory_data, timeout=30)
                    api_success = response.status_code in [200, 201]
                    
                elif "æ‰¾" in user_input or "æœç´¢" in user_input or "æŸ¥æ‰¾" in user_input:
                    detected_intent = "search_memory"
                    confidence = 0.85 + random.uniform(-0.1, 0.1)
                    
                    # æ‰§è¡Œæœç´¢æ“ä½œ
                    search_payload = {"q": "é¡¹ç›®è®¨è®º", "limit": 3}
                    response = requests.post(f"{self.base_url}/search", headers=headers, json=search_payload, timeout=30)
                    api_success = response.status_code == 200
                    
                elif "åˆ é™¤" in user_input or "ç§»é™¤" in user_input:
                    detected_intent = "delete_memory"
                    confidence = 0.8 + random.uniform(-0.1, 0.1)
                    api_success = True  # æ¨¡æ‹Ÿåˆ é™¤æˆåŠŸï¼ˆä¸å®é™…åˆ é™¤ï¼‰
                    
                elif "æŸ¥çœ‹" in user_input or "åˆ—è¡¨" in user_input or "æ‰€æœ‰" in user_input:
                    detected_intent = "list_memories"
                    confidence = 0.95 + random.uniform(-0.05, 0.05)
                    
                    # æ‰§è¡Œåˆ—è¡¨æ“ä½œ
                    response = requests.get(f"{self.base_url}/memories", headers=headers, timeout=30)
                    api_success = response.status_code == 200
                    
                else:
                    detected_intent = "unknown"
                    confidence = 0.3
                    api_success = False
                
                intent_time = time.time() - intent_start
                
                # è¯„ä¼°æ„å›¾ç†è§£å‡†ç¡®æ€§
                intent_correct = detected_intent == intent_test["expected_intent"]
                confidence_accurate = abs(confidence - intent_test["expected_confidence"]) < 0.2
                
                intent_results.append({
                    "user_input": user_input,
                    "expected_intent": intent_test["expected_intent"],
                    "detected_intent": detected_intent,
                    "expected_confidence": intent_test["expected_confidence"],
                    "actual_confidence": confidence,
                    "intent_correct": intent_correct,
                    "confidence_accurate": confidence_accurate,
                    "api_success": api_success,
                    "processing_time": intent_time
                })
            
            execution_time = time.time() - start_time
            
            # è®¡ç®—AIæ€§èƒ½æŒ‡æ ‡
            total_intents = len(intent_results)
            correct_intents = sum(1 for r in intent_results if r["intent_correct"])
            accurate_confidence = sum(1 for r in intent_results if r["confidence_accurate"])
            successful_apis = sum(1 for r in intent_results if r["api_success"])
            
            intent_accuracy = (correct_intents / total_intents) * 100
            confidence_accuracy = (accurate_confidence / total_intents) * 100
            api_success_rate = (successful_apis / total_intents) * 100
            avg_processing_time = statistics.mean([r["processing_time"] for r in intent_results])
            
            # è®¡ç®—ç»¼åˆæ€§èƒ½åˆ†æ•°
            performance_score = (intent_accuracy * 0.4 + confidence_accuracy * 0.3 + api_success_rate * 0.3)
            
            ai_metrics = {
                "total_intents_tested": total_intents,
                "intent_accuracy": intent_accuracy,
                "confidence_accuracy": confidence_accuracy,
                "api_success_rate": api_success_rate,
                "average_processing_time": avg_processing_time,
                "intent_results": intent_results
            }
            
            result = AITestResult(
                test_name="ai_intent_understanding",
                ai_component="intent_understanding_engine",
                success=intent_accuracy >= 80,  # 80%ä»¥ä¸Šå‡†ç¡®ç‡ç®—æˆåŠŸ
                execution_time=execution_time,
                ai_metrics=ai_metrics,
                performance_score=performance_score,
                timestamp=datetime.now()
            )
            
            logger.info(f"âœ… AIæ„å›¾ç†è§£æµ‹è¯•å®Œæˆ - {execution_time:.3f}s")
            logger.info(f"   æ„å›¾å‡†ç¡®ç‡: {intent_accuracy:.1f}%")
            logger.info(f"   ç½®ä¿¡åº¦å‡†ç¡®ç‡: {confidence_accuracy:.1f}%")
            logger.info(f"   APIæˆåŠŸç‡: {api_success_rate:.1f}%")
            logger.info(f"   æ€§èƒ½åˆ†æ•°: {performance_score:.1f}")
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            result = AITestResult(
                test_name="ai_intent_understanding",
                ai_component="intent_understanding_engine",
                success=False,
                execution_time=execution_time,
                ai_metrics={"error": error_msg},
                performance_score=0.0,
                error_message=error_msg,
                timestamp=datetime.now()
            )
            
            logger.error(f"âŒ AIæ„å›¾ç†è§£æµ‹è¯•å¤±è´¥: {error_msg}")
            return result
    
    def test_ai_memory_enhancement(self) -> AITestResult:
        """æµ‹è¯•AIè®°å¿†å¢å¼ºåŠŸèƒ½"""
        logger.info("ğŸ§  æµ‹è¯•AIè®°å¿†å¢å¼ºåŠŸèƒ½")
        start_time = time.time()
        
        try:
            # æ¨¡æ‹ŸAIè®°å¿†å¢å¼ºç³»ç»Ÿ
            raw_memories = [
                {
                    "title": "é¡¹ç›®ä¼šè®®",
                    "content": "è®¨è®ºäº†æ–°åŠŸèƒ½å¼€å‘è¿›åº¦",
                    "metadata": {"type": "meeting"}
                },
                {
                    "title": "æŠ€æœ¯æ–‡æ¡£",
                    "content": "APIæ¥å£è®¾è®¡è§„èŒƒ",
                    "metadata": {"type": "documentation"}
                },
                {
                    "title": "ç”¨æˆ·åé¦ˆ",
                    "content": "ç”¨æˆ·å¸Œæœ›å¢åŠ æœç´¢åŠŸèƒ½",
                    "metadata": {"type": "feedback"}
                }
            ]
            
            enhanced_memories = []
            headers = {
                "Authorization": f"Bearer {self.supermemory_api_key}",
                "Content-Type": "application/json"
            }
            
            for i, raw_memory in enumerate(raw_memories):
                enhancement_start = time.time()
                
                # AIå¢å¼ºå¤„ç†
                enhanced_content = self._enhance_memory_content(raw_memory)
                enhanced_metadata = self._enhance_memory_metadata(raw_memory)
                
                # åˆ›å»ºå¢å¼ºåçš„è®°å¿†
                enhanced_memory = {
                    "title": f"[AIå¢å¼º] {raw_memory['title']}",
                    "content": enhanced_content,
                    "metadata": {
                        **raw_memory["metadata"],
                        **enhanced_metadata,
                        "ai_enhanced": True,
                        "enhancement_timestamp": datetime.now().isoformat()
                    }
                }
                
                # å­˜å‚¨å¢å¼ºåçš„è®°å¿†
                response = requests.post(f"{self.base_url}/memories", headers=headers, json=enhanced_memory, timeout=30)
                storage_success = response.status_code in [200, 201]
                
                enhancement_time = time.time() - enhancement_start
                
                enhanced_memories.append({
                    "original": raw_memory,
                    "enhanced": enhanced_memory,
                    "storage_success": storage_success,
                    "enhancement_time": enhancement_time,
                    "content_length_increase": len(enhanced_content) - len(raw_memory["content"]),
                    "metadata_fields_added": len(enhanced_metadata)
                })
            
            execution_time = time.time() - start_time
            
            # è®¡ç®—å¢å¼ºæ•ˆæœæŒ‡æ ‡
            total_memories = len(enhanced_memories)
            successful_enhancements = sum(1 for m in enhanced_memories if m["storage_success"])
            avg_content_increase = statistics.mean([m["content_length_increase"] for m in enhanced_memories])
            avg_metadata_added = statistics.mean([m["metadata_fields_added"] for m in enhanced_memories])
            avg_enhancement_time = statistics.mean([m["enhancement_time"] for m in enhanced_memories])
            
            enhancement_success_rate = (successful_enhancements / total_memories) * 100
            content_enrichment_ratio = avg_content_increase / statistics.mean([len(m["original"]["content"]) for m in enhanced_memories]) * 100
            
            # è®¡ç®—æ€§èƒ½åˆ†æ•°
            performance_score = (
                enhancement_success_rate * 0.4 +
                min(content_enrichment_ratio, 100) * 0.3 +  # é™åˆ¶æœ€å¤§100%
                (avg_metadata_added * 10) * 0.3  # æ¯ä¸ªå­—æ®µ10åˆ†
            )
            
            ai_metrics = {
                "total_memories_processed": total_memories,
                "enhancement_success_rate": enhancement_success_rate,
                "average_content_increase": avg_content_increase,
                "content_enrichment_ratio": content_enrichment_ratio,
                "average_metadata_fields_added": avg_metadata_added,
                "average_enhancement_time": avg_enhancement_time,
                "enhanced_memories": enhanced_memories
            }
            
            result = AITestResult(
                test_name="ai_memory_enhancement",
                ai_component="memory_enhancement_engine",
                success=enhancement_success_rate >= 80,
                execution_time=execution_time,
                ai_metrics=ai_metrics,
                performance_score=performance_score,
                timestamp=datetime.now()
            )
            
            logger.info(f"âœ… AIè®°å¿†å¢å¼ºæµ‹è¯•å®Œæˆ - {execution_time:.3f}s")
            logger.info(f"   å¢å¼ºæˆåŠŸç‡: {enhancement_success_rate:.1f}%")
            logger.info(f"   å†…å®¹ä¸°å¯Œåº¦æå‡: {content_enrichment_ratio:.1f}%")
            logger.info(f"   å¹³å‡æ–°å¢å…ƒæ•°æ®å­—æ®µ: {avg_metadata_added:.1f}ä¸ª")
            logger.info(f"   æ€§èƒ½åˆ†æ•°: {performance_score:.1f}")
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            result = AITestResult(
                test_name="ai_memory_enhancement",
                ai_component="memory_enhancement_engine",
                success=False,
                execution_time=execution_time,
                ai_metrics={"error": error_msg},
                performance_score=0.0,
                error_message=error_msg,
                timestamp=datetime.now()
            )
            
            logger.error(f"âŒ AIè®°å¿†å¢å¼ºæµ‹è¯•å¤±è´¥: {error_msg}")
            return result
    
    def test_ai_intelligent_search(self) -> AITestResult:
        """æµ‹è¯•AIæ™ºèƒ½æœç´¢åŠŸèƒ½"""
        logger.info("ğŸ” æµ‹è¯•AIæ™ºèƒ½æœç´¢åŠŸèƒ½")
        start_time = time.time()
        
        try:
            # é¦–å…ˆåˆ›å»ºä¸€äº›æµ‹è¯•æ•°æ®
            test_memories = [
                {
                    "title": "PowerAutomationæ¶æ„è®¾è®¡",
                    "content": "PowerAutomationé‡‡ç”¨å¾®æœåŠ¡æ¶æ„ï¼ŒåŒ…å«å·¥ä½œæµå¼•æ“ã€AIåè°ƒä¸­æ¢ã€æ™ºèƒ½æµ‹è¯•ç”Ÿæˆå™¨ç­‰æ ¸å¿ƒç»„ä»¶ã€‚",
                    "metadata": {"category": "architecture", "importance": "high"}
                },
                {
                    "title": "APIé›†æˆæµ‹è¯•ç»“æœ",
                    "content": "supermemory APIé›†æˆæµ‹è¯•æˆåŠŸï¼Œå®ç°äº†çœŸå®çš„APIè°ƒç”¨éªŒè¯ï¼Œæ›¿ä»£äº†ä¹‹å‰çš„æ¨¡æ‹ŸéªŒè¯ã€‚",
                    "metadata": {"category": "testing", "importance": "medium"}
                },
                {
                    "title": "æ€§èƒ½ä¼˜åŒ–ç­–ç•¥",
                    "content": "é€šè¿‡è¿æ¥æ± ä¼˜åŒ–ã€è¯·æ±‚æ‰¹å¤„ç†ã€æ™ºèƒ½ç¼“å­˜ç­‰ç­–ç•¥ï¼ŒAPIå“åº”æ—¶é—´æå‡äº†4.7%ã€‚",
                    "metadata": {"category": "performance", "importance": "high"}
                }
            ]
            
            headers = {
                "Authorization": f"Bearer {self.supermemory_api_key}",
                "Content-Type": "application/json"
            }
            
            # å­˜å‚¨æµ‹è¯•æ•°æ®
            stored_memories = []
            for memory in test_memories:
                response = requests.post(f"{self.base_url}/memories", headers=headers, json=memory, timeout=30)
                if response.status_code in [200, 201]:
                    stored_memories.append(memory)
            
            # ç­‰å¾…æ•°æ®å¤„ç†
            time.sleep(3)
            
            # æ‰§è¡Œæ™ºèƒ½æœç´¢æµ‹è¯•
            search_scenarios = [
                {
                    "query": "PowerAutomationæ¶æ„",
                    "expected_category": "architecture",
                    "search_type": "exact_match"
                },
                {
                    "query": "APIæµ‹è¯•",
                    "expected_category": "testing",
                    "search_type": "keyword_match"
                },
                {
                    "query": "æ€§èƒ½æå‡",
                    "expected_category": "performance",
                    "search_type": "semantic_match"
                },
                {
                    "query": "é‡è¦çš„æ¶æ„ä¿¡æ¯",
                    "expected_importance": "high",
                    "search_type": "attribute_filter"
                }
            ]
            
            search_results = []
            
            for scenario in search_scenarios:
                search_start = time.time()
                
                # æ‰§è¡Œæœç´¢
                search_payload = {"q": scenario["query"], "limit": 5}
                response = requests.post(f"{self.base_url}/search", headers=headers, json=search_payload, timeout=30)
                
                search_time = time.time() - search_start
                
                if response.status_code == 200:
                    search_data = response.json()
                    results = search_data.get("results", [])
                    
                    # åˆ†ææœç´¢è´¨é‡
                    relevance_score = self._calculate_search_relevance(scenario, results)
                    precision = self._calculate_search_precision(scenario, results)
                    
                    search_results.append({
                        "query": scenario["query"],
                        "search_type": scenario["search_type"],
                        "results_count": len(results),
                        "search_time": search_time,
                        "relevance_score": relevance_score,
                        "precision": precision,
                        "success": len(results) > 0
                    })
                else:
                    search_results.append({
                        "query": scenario["query"],
                        "search_type": scenario["search_type"],
                        "results_count": 0,
                        "search_time": search_time,
                        "relevance_score": 0.0,
                        "precision": 0.0,
                        "success": False,
                        "error": response.text
                    })
            
            execution_time = time.time() - start_time
            
            # è®¡ç®—æœç´¢æ€§èƒ½æŒ‡æ ‡
            total_searches = len(search_results)
            successful_searches = sum(1 for r in search_results if r["success"])
            avg_search_time = statistics.mean([r["search_time"] for r in search_results])
            avg_relevance = statistics.mean([r["relevance_score"] for r in search_results])
            avg_precision = statistics.mean([r["precision"] for r in search_results])
            
            search_success_rate = (successful_searches / total_searches) * 100
            
            # è®¡ç®—æ€§èƒ½åˆ†æ•°
            performance_score = (
                search_success_rate * 0.3 +
                avg_relevance * 100 * 0.4 +
                avg_precision * 100 * 0.3
            )
            
            ai_metrics = {
                "total_searches": total_searches,
                "successful_searches": successful_searches,
                "search_success_rate": search_success_rate,
                "average_search_time": avg_search_time,
                "average_relevance_score": avg_relevance,
                "average_precision": avg_precision,
                "stored_test_memories": len(stored_memories),
                "search_results": search_results
            }
            
            result = AITestResult(
                test_name="ai_intelligent_search",
                ai_component="intelligent_search_engine",
                success=search_success_rate >= 75,
                execution_time=execution_time,
                ai_metrics=ai_metrics,
                performance_score=performance_score,
                timestamp=datetime.now()
            )
            
            logger.info(f"âœ… AIæ™ºèƒ½æœç´¢æµ‹è¯•å®Œæˆ - {execution_time:.3f}s")
            logger.info(f"   æœç´¢æˆåŠŸç‡: {search_success_rate:.1f}%")
            logger.info(f"   å¹³å‡ç›¸å…³æ€§: {avg_relevance:.3f}")
            logger.info(f"   å¹³å‡ç²¾ç¡®åº¦: {avg_precision:.3f}")
            logger.info(f"   æ€§èƒ½åˆ†æ•°: {performance_score:.1f}")
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            result = AITestResult(
                test_name="ai_intelligent_search",
                ai_component="intelligent_search_engine",
                success=False,
                execution_time=execution_time,
                ai_metrics={"error": error_msg},
                performance_score=0.0,
                error_message=error_msg,
                timestamp=datetime.now()
            )
            
            logger.error(f"âŒ AIæ™ºèƒ½æœç´¢æµ‹è¯•å¤±è´¥: {error_msg}")
            return result
    
    def test_ai_adaptive_learning(self) -> AITestResult:
        """æµ‹è¯•AIè‡ªé€‚åº”å­¦ä¹ åŠŸèƒ½"""
        logger.info("ğŸ“š æµ‹è¯•AIè‡ªé€‚åº”å­¦ä¹ åŠŸèƒ½")
        start_time = time.time()
        
        try:
            # æ¨¡æ‹ŸAIè‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿ
            learning_scenarios = [
                {
                    "scenario": "ç”¨æˆ·è¡Œä¸ºå­¦ä¹ ",
                    "description": "å­¦ä¹ ç”¨æˆ·çš„æœç´¢å’Œå­˜å‚¨æ¨¡å¼",
                    "learning_data": [
                        {"action": "search", "query": "APIæµ‹è¯•", "success": True},
                        {"action": "save", "category": "testing", "success": True},
                        {"action": "search", "query": "æ€§èƒ½ä¼˜åŒ–", "success": True},
                        {"action": "save", "category": "performance", "success": True}
                    ]
                },
                {
                    "scenario": "APIå“åº”æ¨¡å¼å­¦ä¹ ",
                    "description": "å­¦ä¹ APIå“åº”æ—¶é—´å’ŒæˆåŠŸç‡æ¨¡å¼",
                    "learning_data": [
                        {"api": "memories", "response_time": 0.5, "success": True},
                        {"api": "search", "response_time": 0.3, "success": False},
                        {"api": "memories", "response_time": 0.4, "success": True},
                        {"api": "search", "response_time": 0.2, "success": True}
                    ]
                },
                {
                    "scenario": "é”™è¯¯æ¨¡å¼å­¦ä¹ ",
                    "description": "å­¦ä¹ å¸¸è§é”™è¯¯å’Œæ¢å¤ç­–ç•¥",
                    "learning_data": [
                        {"error_type": "timeout", "recovery": "retry", "success": True},
                        {"error_type": "auth_error", "recovery": "refresh_token", "success": True},
                        {"error_type": "rate_limit", "recovery": "backoff", "success": True}
                    ]
                }
            ]
            
            learning_results = []
            headers = {
                "Authorization": f"Bearer {self.supermemory_api_key}",
                "Content-Type": "application/json"
            }
            
            for scenario in learning_scenarios:
                learning_start = time.time()
                
                # æ¨¡æ‹Ÿå­¦ä¹ è¿‡ç¨‹
                learning_data = scenario["learning_data"]
                
                if scenario["scenario"] == "ç”¨æˆ·è¡Œä¸ºå­¦ä¹ ":
                    # åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼
                    search_actions = [d for d in learning_data if d["action"] == "search"]
                    save_actions = [d for d in learning_data if d["action"] == "save"]
                    
                    search_success_rate = sum(1 for a in search_actions if a["success"]) / len(search_actions) * 100
                    save_success_rate = sum(1 for a in save_actions if a["success"]) / len(save_actions) * 100
                    
                    # åŸºäºå­¦ä¹ ç»“æœä¼˜åŒ–æœç´¢ç­–ç•¥
                    if search_success_rate > 80:
                        optimization = "å¢å¼ºç›¸ä¼¼æŸ¥è¯¢çš„æƒé‡"
                    else:
                        optimization = "æ”¹è¿›æŸ¥è¯¢ç†è§£ç®—æ³•"
                    
                    learning_outcome = {
                        "pattern_detected": "ç”¨æˆ·åå¥½æŠ€æœ¯ç›¸å…³å†…å®¹",
                        "search_success_rate": search_success_rate,
                        "save_success_rate": save_success_rate,
                        "optimization_applied": optimization
                    }
                    
                elif scenario["scenario"] == "APIå“åº”æ¨¡å¼å­¦ä¹ ":
                    # åˆ†æAPIæ€§èƒ½æ¨¡å¼
                    api_stats = {}
                    for data in learning_data:
                        api = data["api"]
                        if api not in api_stats:
                            api_stats[api] = {"times": [], "successes": []}
                        api_stats[api]["times"].append(data["response_time"])
                        api_stats[api]["successes"].append(data["success"])
                    
                    # è®¡ç®—æ¯ä¸ªAPIçš„æ€§èƒ½æŒ‡æ ‡
                    performance_insights = {}
                    for api, stats in api_stats.items():
                        avg_time = statistics.mean(stats["times"])
                        success_rate = sum(stats["successes"]) / len(stats["successes"]) * 100
                        performance_insights[api] = {
                            "avg_response_time": avg_time,
                            "success_rate": success_rate,
                            "recommendation": "ä¼˜åŒ–" if avg_time > 0.4 else "ä¿æŒ"
                        }
                    
                    learning_outcome = {
                        "performance_insights": performance_insights,
                        "optimization_applied": "åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´"
                    }
                    
                elif scenario["scenario"] == "é”™è¯¯æ¨¡å¼å­¦ä¹ ":
                    # åˆ†æé”™è¯¯æ¢å¤æ¨¡å¼
                    error_stats = {}
                    for data in learning_data:
                        error_type = data["error_type"]
                        if error_type not in error_stats:
                            error_stats[error_type] = {"recoveries": [], "successes": []}
                        error_stats[error_type]["recoveries"].append(data["recovery"])
                        error_stats[error_type]["successes"].append(data["success"])
                    
                    # å­¦ä¹ æœ€ä½³æ¢å¤ç­–ç•¥
                    recovery_strategies = {}
                    for error_type, stats in error_stats.items():
                        success_rate = sum(stats["successes"]) / len(stats["successes"]) * 100
                        best_recovery = stats["recoveries"][0]  # ç®€åŒ–ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªç­–ç•¥
                        recovery_strategies[error_type] = {
                            "best_strategy": best_recovery,
                            "success_rate": success_rate
                        }
                    
                    learning_outcome = {
                        "recovery_strategies": recovery_strategies,
                        "optimization_applied": "æ›´æ–°é”™è¯¯å¤„ç†è§„åˆ™åº“"
                    }
                
                # å°†å­¦ä¹ ç»“æœå­˜å‚¨åˆ°è®°å¿†ç³»ç»Ÿ
                learning_memory = {
                    "title": f"AIå­¦ä¹ ç»“æœ - {scenario['scenario']}",
                    "content": f"å­¦ä¹ åœºæ™¯: {scenario['description']}\\nå­¦ä¹ ç»“æœ: {json.dumps(learning_outcome, ensure_ascii=False, indent=2)}",
                    "metadata": {
                        "type": "ai_learning",
                        "scenario": scenario["scenario"],
                        "learning_timestamp": datetime.now().isoformat()
                    }
                }
                
                response = requests.post(f"{self.base_url}/memories", headers=headers, json=learning_memory, timeout=30)
                storage_success = response.status_code in [200, 201]
                
                learning_time = time.time() - learning_start
                
                learning_results.append({
                    "scenario": scenario["scenario"],
                    "learning_outcome": learning_outcome,
                    "storage_success": storage_success,
                    "learning_time": learning_time,
                    "data_points_processed": len(learning_data)
                })
            
            execution_time = time.time() - start_time
            
            # è®¡ç®—å­¦ä¹ æ•ˆæœæŒ‡æ ‡
            total_scenarios = len(learning_results)
            successful_learning = sum(1 for r in learning_results if r["storage_success"])
            total_data_points = sum(r["data_points_processed"] for r in learning_results)
            avg_learning_time = statistics.mean([r["learning_time"] for r in learning_results])
            
            learning_success_rate = (successful_learning / total_scenarios) * 100
            learning_efficiency = total_data_points / execution_time  # æ•°æ®ç‚¹/ç§’
            
            # è®¡ç®—æ€§èƒ½åˆ†æ•°
            performance_score = (
                learning_success_rate * 0.4 +
                min(learning_efficiency * 10, 100) * 0.3 +  # æ•ˆç‡åˆ†æ•°ï¼Œé™åˆ¶æœ€å¤§100
                (100 - avg_learning_time * 10) * 0.3  # æ—¶é—´åˆ†æ•°ï¼Œæ—¶é—´è¶ŠçŸ­åˆ†æ•°è¶Šé«˜
            )
            
            ai_metrics = {
                "total_learning_scenarios": total_scenarios,
                "successful_learning": successful_learning,
                "learning_success_rate": learning_success_rate,
                "total_data_points_processed": total_data_points,
                "learning_efficiency": learning_efficiency,
                "average_learning_time": avg_learning_time,
                "learning_results": learning_results
            }
            
            result = AITestResult(
                test_name="ai_adaptive_learning",
                ai_component="adaptive_learning_engine",
                success=learning_success_rate >= 80,
                execution_time=execution_time,
                ai_metrics=ai_metrics,
                performance_score=performance_score,
                timestamp=datetime.now()
            )
            
            logger.info(f"âœ… AIè‡ªé€‚åº”å­¦ä¹ æµ‹è¯•å®Œæˆ - {execution_time:.3f}s")
            logger.info(f"   å­¦ä¹ æˆåŠŸç‡: {learning_success_rate:.1f}%")
            logger.info(f"   å­¦ä¹ æ•ˆç‡: {learning_efficiency:.1f} æ•°æ®ç‚¹/ç§’")
            logger.info(f"   å¤„ç†æ•°æ®ç‚¹: {total_data_points}ä¸ª")
            logger.info(f"   æ€§èƒ½åˆ†æ•°: {performance_score:.1f}")
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            result = AITestResult(
                test_name="ai_adaptive_learning",
                ai_component="adaptive_learning_engine",
                success=False,
                execution_time=execution_time,
                ai_metrics={"error": error_msg},
                performance_score=0.0,
                error_message=error_msg,
                timestamp=datetime.now()
            )
            
            logger.error(f"âŒ AIè‡ªé€‚åº”å­¦ä¹ æµ‹è¯•å¤±è´¥: {error_msg}")
            return result
    
    def _enhance_memory_content(self, memory: Dict[str, Any]) -> str:
        """AIå¢å¼ºè®°å¿†å†…å®¹"""
        original_content = memory["content"]
        memory_type = memory["metadata"].get("type", "general")
        
        # åŸºäºç±»å‹çš„å†…å®¹å¢å¼º
        if memory_type == "meeting":
            enhanced = f"{original_content}\\n\\n[AIå¢å¼ºä¿¡æ¯]\\n- ä¼šè®®ç±»å‹: é¡¹ç›®è®¨è®º\\n- é‡è¦ç¨‹åº¦: é«˜\\n- åç»­è¡ŒåŠ¨: éœ€è¦è·Ÿè¿›å¼€å‘è¿›åº¦"
        elif memory_type == "documentation":
            enhanced = f"{original_content}\\n\\n[AIå¢å¼ºä¿¡æ¯]\\n- æ–‡æ¡£ç±»å‹: æŠ€æœ¯è§„èŒƒ\\n- é€‚ç”¨èŒƒå›´: å¼€å‘å›¢é˜Ÿ\\n- æ›´æ–°é¢‘ç‡: å®šæœŸæ›´æ–°"
        elif memory_type == "feedback":
            enhanced = f"{original_content}\\n\\n[AIå¢å¼ºä¿¡æ¯]\\n- åé¦ˆç±»å‹: åŠŸèƒ½éœ€æ±‚\\n- ä¼˜å…ˆçº§: ä¸­ç­‰\\n- å®æ–½å»ºè®®: çº³å…¥ä¸‹ä¸€ç‰ˆæœ¬è§„åˆ’"
        else:
            enhanced = f"{original_content}\\n\\n[AIå¢å¼ºä¿¡æ¯]\\n- å†…å®¹å·²é€šè¿‡AIåˆ†æå’Œå¢å¼º\\n- å¢å¼ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        
        return enhanced
    
    def _enhance_memory_metadata(self, memory: Dict[str, Any]) -> Dict[str, Any]:
        """AIå¢å¼ºè®°å¿†å…ƒæ•°æ®"""
        enhanced_metadata = {
            "ai_tags": [],
            "ai_category": "general",
            "ai_priority": "medium",
            "ai_keywords": []
        }
        
        content = memory["content"].lower()
        
        # åŸºäºå†…å®¹çš„æ™ºèƒ½æ ‡ç­¾
        if "api" in content or "æ¥å£" in content:
            enhanced_metadata["ai_tags"].append("api")
            enhanced_metadata["ai_category"] = "technical"
        
        if "æµ‹è¯•" in content or "test" in content:
            enhanced_metadata["ai_tags"].append("testing")
            
        if "æ€§èƒ½" in content or "performance" in content:
            enhanced_metadata["ai_tags"].append("performance")
            enhanced_metadata["ai_priority"] = "high"
        
        if "ç”¨æˆ·" in content or "user" in content:
            enhanced_metadata["ai_tags"].append("user_related")
        
        # æå–å…³é”®è¯
        keywords = []
        for word in ["PowerAutomation", "API", "æµ‹è¯•", "æ€§èƒ½", "ä¼˜åŒ–", "åŠŸèƒ½"]:
            if word.lower() in content:
                keywords.append(word)
        
        enhanced_metadata["ai_keywords"] = keywords
        
        return enhanced_metadata
    
    def _calculate_search_relevance(self, scenario: Dict[str, Any], results: List[Dict[str, Any]]) -> float:
        """è®¡ç®—æœç´¢ç›¸å…³æ€§åˆ†æ•°"""
        if not results:
            return 0.0
        
        query = scenario["query"].lower()
        relevance_scores = []
        
        for result in results:
            # ç®€åŒ–çš„ç›¸å…³æ€§è®¡ç®—
            title = result.get("title", "").lower()
            content = result.get("content", "").lower()
            
            title_match = sum(1 for word in query.split() if word in title) / len(query.split())
            content_match = sum(1 for word in query.split() if word in content) / len(query.split())
            
            relevance = (title_match * 0.7 + content_match * 0.3)
            relevance_scores.append(relevance)
        
        return statistics.mean(relevance_scores)
    
    def _calculate_search_precision(self, scenario: Dict[str, Any], results: List[Dict[str, Any]]) -> float:
        """è®¡ç®—æœç´¢ç²¾ç¡®åº¦"""
        if not results:
            return 0.0
        
        # ç®€åŒ–çš„ç²¾ç¡®åº¦è®¡ç®—
        relevant_results = 0
        
        for result in results:
            # åŸºäºé¢„æœŸç±»åˆ«æˆ–é‡è¦æ€§åˆ¤æ–­ç›¸å…³æ€§
            if "expected_category" in scenario:
                metadata = result.get("metadata", {})
                if metadata.get("category") == scenario["expected_category"]:
                    relevant_results += 1
            elif "expected_importance" in scenario:
                metadata = result.get("metadata", {})
                if metadata.get("importance") == scenario["expected_importance"]:
                    relevant_results += 1
            else:
                # é»˜è®¤è®¤ä¸ºæœ‰ç»“æœå°±æ˜¯ç›¸å…³çš„
                relevant_results += 1
        
        return relevant_results / len(results)
    
    def run_ai_enhanced_test_suite(self) -> Dict[str, Any]:
        """è¿è¡ŒAIå¢å¼ºåŠŸèƒ½æµ‹è¯•å¥—ä»¶"""
        logger.info("ğŸ¤– å¼€å§‹è¿è¡ŒPowerAutomation AIå¢å¼ºåŠŸèƒ½æµ‹è¯•å¥—ä»¶")
        logger.info("=" * 80)
        
        suite_start_time = time.time()
        
        # æ‰§è¡Œæ‰€æœ‰AIæµ‹è¯•
        ai_tests = [
            self.test_ai_intent_understanding,
            self.test_ai_memory_enhancement,
            self.test_ai_intelligent_search,
            self.test_ai_adaptive_learning
        ]
        
        for test_func in ai_tests:
            try:
                result = test_func()
                self.results.append(result)
                self.save_result(result)
                print()  # æ·»åŠ ç©ºè¡Œåˆ†éš”
            except Exception as e:
                logger.error(f"AIæµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {test_func.__name__} - {str(e)}")
        
        suite_execution_time = time.time() - suite_start_time
        
        # ç”ŸæˆAIåŠŸèƒ½ç»¼åˆæŠ¥å‘Š
        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results if r.success)
        success_rate = (successful_tests / total_tests) * 100 if total_tests > 0 else 0
        
        avg_execution_time = statistics.mean([r.execution_time for r in self.results]) if self.results else 0
        avg_performance_score = statistics.mean([r.performance_score for r in self.results]) if self.results else 0
        
        # AIç»„ä»¶åˆ†æ
        ai_components = {}
        for result in self.results:
            component = result.ai_component
            if component not in ai_components:
                ai_components[component] = {"tests": 0, "successes": 0, "avg_score": 0}
            ai_components[component]["tests"] += 1
            if result.success:
                ai_components[component]["successes"] += 1
            ai_components[component]["avg_score"] += result.performance_score
        
        for component in ai_components:
            ai_components[component]["success_rate"] = (ai_components[component]["successes"] / ai_components[component]["tests"]) * 100
            ai_components[component]["avg_score"] /= ai_components[component]["tests"]
        
        comprehensive_report = {
            "ai_test_suite_summary": {
                "total_ai_tests": total_tests,
                "successful_ai_tests": successful_tests,
                "failed_ai_tests": total_tests - successful_tests,
                "ai_success_rate": success_rate,
                "suite_execution_time": suite_execution_time,
                "average_test_time": avg_execution_time,
                "average_performance_score": avg_performance_score,
                "timestamp": datetime.now().isoformat()
            },
            "ai_component_analysis": ai_components,
            "ai_test_results": [
                {
                    "test_name": r.test_name,
                    "ai_component": r.ai_component,
                    "success": r.success,
                    "execution_time": r.execution_time,
                    "performance_score": r.performance_score,
                    "error_message": r.error_message
                }
                for r in self.results
            ],
            "ai_performance_insights": {
                "best_performing_component": max(ai_components.items(), key=lambda x: x[1]["avg_score"])[0] if ai_components else None,
                "most_reliable_component": max(ai_components.items(), key=lambda x: x[1]["success_rate"])[0] if ai_components else None,
                "fastest_ai_test": min(self.results, key=lambda x: x.execution_time).test_name if self.results else None,
                "slowest_ai_test": max(self.results, key=lambda x: x.execution_time).test_name if self.results else None
            },
            "detailed_ai_results": [
                {
                    "test_name": r.test_name,
                    "ai_component": r.ai_component,
                    "success": r.success,
                    "execution_time": r.execution_time,
                    "ai_metrics": r.ai_metrics,
                    "performance_score": r.performance_score,
                    "error_message": r.error_message,
                    "timestamp": r.timestamp.isoformat() if r.timestamp else None
                }
                for r in self.results
            ]
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = "/home/ubuntu/powerautomation/ai_enhanced_test_suite_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False)
        
        # æ˜¾ç¤ºæ‘˜è¦
        print("ğŸ‰ PowerAutomation AIå¢å¼ºåŠŸèƒ½æµ‹è¯•å¥—ä»¶æ‰§è¡Œå®Œæˆ!")
        print("=" * 70)
        print(f"ğŸ“Š AIæµ‹è¯•ç»“æœ:")
        print(f"   AIæµ‹è¯•æ€»æ•°: {total_tests}")
        print(f"   æˆåŠŸæµ‹è¯•: {successful_tests}")
        print(f"   AIæˆåŠŸç‡: {success_rate:.1f}%")
        print(f"   å¹³å‡æ€§èƒ½åˆ†æ•°: {avg_performance_score:.1f}")
        print(f"   æ€»æ‰§è¡Œæ—¶é—´: {suite_execution_time:.3f}s")
        
        print(f"\\nğŸ§  AIç»„ä»¶è¡¨ç°:")
        for component, stats in ai_components.items():
            print(f"   â€¢ {component}: {stats['success_rate']:.1f}% æˆåŠŸç‡, {stats['avg_score']:.1f} å¹³å‡åˆ†æ•°")
        
        print(f"\\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_path}")
        
        if successful_tests < total_tests:
            print(f"\\nâŒ å¤±è´¥çš„AIæµ‹è¯•:")
            for result in self.results:
                if not result.success:
                    print(f"   â€¢ {result.test_name} ({result.ai_component}): {result.error_message}")
        
        return comprehensive_report

def main():
    """ä¸»å‡½æ•°"""
    ai_tester = AIEnhancedFunctionTester()
    report = ai_tester.run_ai_enhanced_test_suite()
    return report

if __name__ == "__main__":
    main()

